2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
FisheyeDistanceNet: Self-Supervised Scale-Aware Distance Estimation
using Monocular Fisheye Camera for Autonomous Driving
Varun Ravi Kumar1, Sandesh Athni Hiremath1, Markus Bach1, Stefan Milz1,
Christian Witt1, CleÂ´ment Pinard2, Senthil Yogamani3 and Patrick MaÂ¨der4
1Valeo DAR Kronach, Germany 2ENSTA ParisTech Palaiseau, France
3Valeo Vision Systems, Ireland 4Technische UniversitaÂ¨t Ilmenau, Germany
Abstractâ€”Fisheye cameras are commonly used in applica-
tions like autonomous driving and surveillance to provide a
â—¦
large ï¬eld of view (> 180 ). However, they come at the cost
of strong non-linear distortions which require more complex
algorithms. In this paper, we explore Euclidean distance es-
timation on ï¬sheye cameras for automotive scenes. Obtaining WoodScape KITTI
accurate and dense depth supervision is difï¬cult in practice,
butself-supervisedlearningapproachesshowpromisingresults
andcouldpotentiallyovercometheproblem.Wepresentanovel
self-supervised scale-aware framework for learning Euclidean
distance and ego-motion from raw monocular ï¬sheye videos
without applying rectiï¬cation. While it is possible to perform
piece-wise linear approximation of ï¬sheye projection surface
and apply standard rectilinear models, it has its own set Fig.1Distanceanddepthderivedfromasingleï¬sheyeimage(left)
of issues like re-sampling distortion and discontinuities in andsinglepinholeimage(right)respectively.Ourself-supervised
transition regions. To encourage further research in this area, model,FisheyeDistanceNet,producessharp,highqualitydistanceand
depthmaps.
we will release our dataset as part of the WoodScape project
[1].WefurtherevaluatedtheproposedalgorithmontheKITTI
dataset and obtained state-of-the-art results comparable to Depth estimation models may be learned in a super-
otherself-supervisedmonocularmethods.Qualitativeresultson vised fashion on LiDAR distance measurements, such as
an unseen ï¬sheye video demonstrate impressive performance1. KITTI[23].Inpreviouswork,wefollowedthisapproachand
demonstratedthepossibilitytoestimatehigh-qualitydistance
maps using LiDAR ground truth on ï¬sheye images [12].
I. INTRODUCTION However, setting up the entire rig for such recordings is
expensive and time consuming, and therefore limits the
There has been a signiï¬cant rise in the usage of ï¬sheye
amount of data on which a model can be trained. To
cameras in various automotive applications [2], [3], [4],
overcome this problem, we propose FisheyeDistanceNet,
surveillance [5] and robotics [6] due to their large Field
the ï¬rst end-to-end self-supervised monocular scale-aware
of View (FOV). Recently, several computer vision tasks
training framework. FisheyeDistanceNet uses convolutional
on ï¬sheye cameras have been explored including object
neural networks (CNN) on raw ï¬sheye image sequences to
detection [7], soiling detection [8], motion estimation [9],
regressaEuclideandistancemapandprovidesabaselinefor
image restoration [10] and SLAM [11]. Depth estimation is
single frame Euclidean distance estimation. We summarize
animportanttaskinautonomousdrivingasitisusedtoavoid
our contributions as follows:
obstacles and plan trajectories. While depth estimation has
been substantially studied for narrow FOV cameras, it has â€¢ A self-supervised training strategy that aims at infer-
barely been explored for ï¬sheye cameras [12], [13]. ring a distance map from a sequence of distorted and
Previous learning-based approaches [14], [15], [16], [17] unrectiï¬ed raw ï¬sheye images.
have solely focused on traditional 2D content captured with â€¢ A solution to the scale factor uncertainty with the bol-
cameras following a typical pinhole projection model based ster from ego-motion velocity allows outputting metric
on rectiï¬ed image sequences. With the surge of efï¬cient distance maps. This facilitates the mapâ€™s practical use
and cheap wide angle ï¬sheye cameras and their larger FOV for self-driving cars.
in contrast to pinhole cameras, there has been signiï¬cant â€¢ A novel combination of super resolution networks and
interest in the computer vision community to perform depth deformable convolution layers [24] to output high reso-
estimationfromomnidirectionalcontentsimilartotraditional lution distance maps with sharp boundaries from a low
2D content via omnidirectional stereo [18], [19], [20], [21] resolutioninput.Inspiredbythesuperresolutionofim-
and structure-from-motion (SfM) [22] approaches. agesapproach[25]thisapproachallowsustoaccurately
resolve distances replacing the deconvolution [26] and
1seeFig.1andhttps://youtu.be/Sgq1WzoOmXg naive nearest neighbor or bilinear upsampling.
978-1-7281-7395-5/20/$31.00 Â©2020 IEEE 574
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. perform image synthesis. Depth estimation is an ill-posed
â„³ğ‘¡ â†’ğ‘¡âˆ’1 â„³ ğ‘¡â†’ğ‘¡+1 problem as there could exist a large number of possible
incorrect depths per pixel, which can also recreate the novel
view, given the relative pose between I and I (cid:48).
t t
Using view-synthesis as the supervising technique we can
train the network using the viewpoint of I âˆ’ and I to
t 1 t+1
estimate the appearance of a target image I on raw ï¬sheye
t
ğ·ğ‘¡âˆ’1   ğ·ğ‘¡ ğ·ğ‘¡+1 images. A naive approach would be correcting raw ï¬sheye
DistanceNet PoseNet DistanceNet PoseNet DistanceNet images to piecewise or cylindrical projections and would
essentially render the problem equivalent to Zhou et al.â€™s
work [15]. In contrast, at the core of our approach there
is a simple yet efï¬cient technique for obtaining scale-aware
distance maps.
This section starts with discussing the geometry of the
Cğ¼ ğ‘¡oâˆ’n1c+ağ¼tğ‘¡  It Cğ¼ğ‘¡o+ncğ¼ğ‘¡a+t 1 problemandhowitisusedtoobtaindifferentiablelosses.We
       ğ¼ğ‘¡âˆ’1  ğ¼ğ‘¡ ğ¼ğ‘¡+1  describe the scale-aware FisheyeDistanceNet and its effects
    Vğ‘¡âˆ’1 Vğ‘¡ Vğ‘¡ Vğ‘¡+1 on the output distance estimates. Additionally, we provide
ğ‘¡âˆ’1 ğ‘¡ ğ‘¡ ğ‘¡+1
an in-depth discussion of the various losses.
Fig.2Overviewofourmethod.Theï¬rstrowrepresentsouregomasks
M M
asdescribedinSectionII-D, â†’âˆ’ , â†’ indicatewhichpixel
t t 1 t t+1 A. Modeling of Fisheye Geometry
coordinatesarevalidwhenconstructingIË†tâˆ’1â†’t fromItâˆ’1 andIË†t+1â†’t
fromIt+1 respectively.Thesecondrowindicatesthemaskingofstatic 1) Projection from camera coordinates to image coordi-
pixelscomputedafter2epochs,whereblackpixelsareï¬lteredfromthe (cid:55)â†’
photometricloss(i.e.Ï‰=0).Itpreventsdynamicobjectsatsimilarspeed nates: The projection function Xc Î (Xc) = p of a 3D
astheegocarandlowtextureregionsfromcontaminatingtheloss.The point X = (x ,y ,z )T in camera coordinates to a pixel
c c c c
masksarecomputedforforwardandbackwardsequencesfromtheinput p = (u,v)T in the image coordinates is obtained via a 4th
sequenceS andreconstructedimagesusingEq.10asdescribedinSection
order polynomial in the following way:
II-D.Thethirdrowrepresentsthedistanceestimatescorrespondingto
theirinputframes.Finally,thevehicleâ€™sodometrydataisusedtoresolve
thescalefactorissue. Ï•=arctan2(y ,x ) (1)
c c
Ï€ âˆ’
Î¸ =(cid:18)2 (cid:19)arct(cid:18)an2(zc,rc) (cid:19) (2)
â€¢ We depict the importance of using backward sequences Â· Â· Â· Â·
(Î¸)=k Î¸+k Î¸2+k Î¸3+k Î¸4 (3)
for training and construct a loss for these sequences. 1 2 Â· 3 Â· 4
Moreover, a combination of ï¬ltering static pixels and p=(cid:112)u = (Î¸)Â·cosÏ•Â·ax+cx (4)
an ego mask is employed. The incorporated bundle- v (Î¸) sinÏ• ay+cy
adjustment framework [27] jointly optimizes distances
and camera poses within a sequence by increasing where rc = x2c +yc2, Î¸ is the angle of incidence,
the baseline and providing additional consistency con- (Î¸) is the mapping of incident angle to image radius,
straints. (ax,ay)istheaspectratioand(cx,cy)istheprincipalpoint.
II. SELF-SUPERVISEDSCALE-AWARE 2) Unprojection from image coordinates to camera coor-
FISHEYEDISTANCENET dinates: The unprojection function (p,DË†) (cid:55)â†’ Î âˆ’1(p,DË†) =
(cid:0)
Zhou et al.â€™s [15] self-supervised monocular structure- X of an image pixel p=(u,v)T and itâ€™s distance estimate
c
from-motion (SfM) framework aims at learning: DË† to th(cid:1)e 3D point X = (x ,y ,z )T is obtained via the
1) a monocular depth model gd : It â†’ D predicting a following steps. Lettincg (xi,cyi)cT c= (u âˆ’ cx)(cid:112)/ax,(v âˆ’
scale-ambiguous depth DË† = gd(It(p)) per pixel p in cy)/ay T,weobtaintheangleofincidenceÎ¸bynumerically
the target image It; and â†’ calculating the 4th order polynomial roots of = x2i +yi2
2) an ego-motion predictor gx : (It,It(cid:48)) Itâ†’t(cid:48) pre- usingthedistortioncoefï¬cientsk1,k2,k3,k4 (seeEq.3).For
dicting a set of six degrees of freedom rigid transfor- training efï¬ciency, we pre-calculate the roots and store them
âˆˆ
mations Ttâ†’t(cid:48) SE(3), between the target image(cid:48) Iâˆˆt inalookuptableforallthepixelcoordinates.Now,Î¸ isused
and the set of reference images I (cid:48). Typically, t to get
{ âˆ’ } t
ast+ref1e,rtence1im, ia.eg.esth,ealftrhaomuegshIutsâˆ’i1nganadlaIrtg+e1rawreinudsoewd rc =DË† Â·sinÎ¸ and(cid:112) zc =DË† Â·cosÎ¸ (5)
is possible. where the distance estimate DË† from the network represents
(cid:107) (cid:107)
A limitation of this approach is that both depth and pose are theEuclideandistance X = x2+y2+z2 ofa3Dpoint
c c c c
estimated up to an unknown scale factor in the monocular X . The polar angle Ï• and the x , y components can be
c c c
SfM pipeline. obtained as follows:
The depth which acts as an intermediary variable is
Â· Â·
obtained from the network by constraining the model to Ï•=arctan2(y ,x ), x =r cosÏ•, y =r sinÏ•.
i i c c c c
575
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. B. Photometric Loss chosentoconstrainD between0.1and100units[14].Fora
Letusconsidertheimagereconstructionerrorfromapair spherical image, we can only obtain angular disparities [31]
of images I (cid:48) and I , distance estimate DË† at time t, and by rectiï¬cation. To perform distance estimation on raw
t t t
the relative pose for I , with respect to the source image ï¬sheye images, we would require metric distance values to
t
It(cid:48)â€™s pose, as Ttâ†’t(cid:48). Using the distance estimate DË†t of the warp the source image It(cid:48) onto the target frame It. Due
network a point cloud P is obtained via: to the limitations of the monocular SfM objective, both the
t
Pt =Î âˆ’1(pt,DË†t) (6) mscoanleo-caumlabrigdueoputshvgadlueasndwehgicoh-mwotoiounldpmreadkiectoirt igmxpopsresidbilcet
âˆ’
whereÎ  1representstheunprojectionfromimagetocamera to estimate distance maps on ï¬sheye images. To achieve
coordinatesasexplainedinSectionII-A.2,p thepixelsetof scale-awaredistancevalues,wenormalizetheposenetworkâ€™s
t
image It. The pose estimate Ttâ†’t(cid:48) from the pose network is estimate Ttâ†’t(cid:48) and scale it with âˆ†x, the displacement
used to get an estimate PË†t(cid:48) =Ttâ†’t(cid:48)Pt for the point cloud of magnitude relative to target frame It which is calculated(cid:48)
theim(cid:48)ageIt(cid:48).PË†t(cid:48) isthenprojectedontotheï¬sheyecameraat usingvehicleâ€™sinstantaneousvelocityestimatesvt(cid:48) attimet
timet usingtheprojectionmodelÎ describedinSectionII- andvt attimet.WealsoapplythistechniqueonKITTI[23]
A.1. Combining transformation and projection with Eq. 6 to obtain metric depth maps.
establishes a mapping from image coordinates p =(u,v)T
t (cid:48) T â†’ (cid:48) Â·
amtatpimpiengt taolliomw(cid:0)asgefocroothredinreacteosnspË†ttr(cid:48)u(cid:1)=cti(ouË†n,vË†IË†)(cid:48)Tâ†’at(cid:68)otifmteh(cid:69)et.taTrgheist Ttâ†’t(cid:48) = (cid:107)Tttâ†’tt(cid:48)(cid:107) âˆ†x (9)
t t
frame I by backward warping the source frame I (cid:48). D. Masking Static Pixels and Ego Mask
t t
âˆ’
pË†(cid:48) =Î  T â†’ (cid:48)Î  1(p ,DË† ) , IË†u(cid:48)vâ†’ = IuË†(cid:48)vË† (7) Following [14], we incorporate a masking approach to
t t t t t t t t
ï¬lter out static pixels which do not change their appearance
Since the warped coordinates uË†,vË† are continuous, we apply from one frame to the other in the training sequence. The
the differentiable spatial transformer network introduced
(cid:10) (cid:11) approach would ï¬lter out objects which move at the same
by[28]tocomputeIË†t(cid:48)â†’tbyperformingbilinearinterpolation speed as the ego-car, and also ignore the static frame when
ofthefourpixelsfromIt(cid:48) whichlieclosetopË†t(cid:48).Thesymbol the ego-car stops moving. Similar to other approaches [14],
... denotes the corresponding sampling operator. [15], [32], [33] the per-pixel mask Ï‰ is applied to the loss
Following [16], [29] the image reconstruction error be-
by weighting the pixels selectively. Instead of being learned
tween the target image It and the reconstructed target im- from the object motion [34], the mask is computed in the
age IË†t(cid:48)â†’t is calculated using the L1 pixel-wise loss term forward pass of the network, yielding a binary mask output
combined with Structural Similarity (SSIM) [30], as our âˆˆ { }
L where Ï‰ 0,1 . Wherever the photometric error of the
photometric loss given by Eq. 8 below.
p warped image IË†(cid:48)â†’ is not lower than that of the original
âˆ’ (cid:13) M (cid:13) t t
LËœp(It,IË†t(cid:48)â†’t)=Î± 1 SSIM(cid:13)(cid:13)(It,2IË†t(cid:48)â†’t, tâ†’t(cid:48)) (cid:13)(cid:13) utanrwgeatrpfreadmseoIu(cid:2)tr,cÏ‰e ifsrasmetetoIti(cid:48)gninoreeatchhelcoassseocfosmucpharp(cid:3)eidxetlos,it.hee.
âˆ’ âˆ’ (cid:12)M
L +(1 Î±) L(It IË†t(cid:48)â†’t) tâ†’t(cid:48) l1 Ï‰ = mti(cid:48)npe(It,IË†t(cid:48)â†’t)<mti(cid:48)npe(It,It(cid:48)) (10)
p = (cid:48)âˆˆ{minâˆ’ } Ëœp(It,IË†t(cid:48)â†’t) (8) where[]istheIversonbracket.Additionally,weaddabinary
t t+1,t 1 M
M ego mask â†’ (cid:48) proposed in [35] that ignores computing
where Î± = 0.85, â†’ (cid:48) is the binary mask as discussed t t
t t (cid:12) the photometric loss on the pixels that do not have a valid
in Section II-D and the symbol denotes element-wise
mapping i.e. some pixel coordinates of the target image I
multiplication. Following [14] instead of averaging the pho- t
may not be projected onto the source image I (cid:48) given the
tometric error over all source images, we adopt per-pixel t
estimated distance DË† .
minimum. This signiï¬cantly sharpens the occlusion bound- t
aries and reduces the artifacts resulting in higher accuracy.
E. Backward Sequence
The self-supervised framework assumes a static scene,
In the forward sequence, we synthesize the target frame
no occlusion and change of appearance (e.g. brightness
constancy). A large photometric cost is incurred, potentially It with the s(cid:48)ouâˆˆrce{frames Iâˆ’tâˆ’1 }and It+1 (i.e. as per above
worsening the performance, if there exist dynamic objects discussion t t + 1,t 1 ). Analogously, backward
and occluded regions. These areas are treated as outliers sequence is carried out by using Itâˆ’1 and It+1 as target
similar to [27] and clip the photometric loss values to a 95th frames and It as source frame. We include warps IË†tâ†’tâˆ’1
percentile. Zero gradient is obtained for errors larger than and IË†tâ†’t+1, thereby inducing more constraints to avoid
overï¬ttingandresolveunknowndistancesintheborderareas
95%. This improves the optimization process and provides a
at the test time, as also observed in previous works [14],
way to strengthen the photometric error.
[15], [36]. We construct the loss for the additional backward
C. Solving Scale Factor Ambiguity at Training Time sequence in a similar manner to the forward. This comes at
âˆ
For a pinhole projection model, depth 1/disparity. thecostofhighcomputationaleffortandlongertrainingtime
Henceforth, the networkâ€™s sigmoided output Ïƒ can be con- asweperformtwoforwardandbackwardwarpswhichyields
verted to depth with D = 1/(aÏƒ+b), where a and b are superior results on the Fisheye and KITTI dataset compared
576
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. to the previous approaches [14], [15] which train only with H. Final Training Loss
one forward sequence and one backward sequence.
The overall self-supervised structure-from-motion (SfM)
L
from motion objective consists of a photometric loss
F. Edge-Aware Smoothness Loss p
imposed between the reconstructed target image IË†(cid:48)â†’ and
t t
In order to regularize distance and avoid divergent values the target image I , included once for the forward and once
t
inoccludedortexture-lesslow-imagegradientareas,weadd for the backward sequence, and a distance regularization
L
a geometric smoothing loss. We adopt the edge-aware term term ensuring edge-aware smoothing in the distance
s L
similarto[16],[35],[37].Theregularizationtermisimposed estimates.Finally, across-sequencedistanceconsistency
dc
ontheinversedistancemap.Unlikepreviousworks,theloss lossderivedfromthechainofframesinthetrainingsequence
isnotdecayedforeachpyramidlevelbyafactorof2dueto S is also included. To prevent the training objective getting
down-sampling, as we use a super resolution network (see stuck in the local minima due to the gradient locality of the
Section III-A) bilinear sampler [28], we adopt 4 scales to train the network
as followed in [15], [16]. The ï¬nal objective function is
L (DË† )=|âˆ‚ DË†âˆ—|eâˆ’|âˆ‚uIt|+|âˆ‚ DË†âˆ—|eâˆ’|âˆ‚vIt| (11) averaged over per(cid:88)-pixel, scale and image batch.
s t u t v t
L
To discourage shrinking of estimated distance [17], mean- L 4
noâˆ’rmalized inverse distance of It is consideâˆ’red, i.e. DË†tâˆ— = = 2nâˆ’n1, (13)
DË† 1/D , where D denotes the mean of DË† 1 :=1/DË† . L n=L1 L L L
t t t t t =n f +n b +Î³ n +Î² n
n p p dc s
G. Cross-Sequence Distance Consistency Loss
{ The SÂ·fÂ·MÂ· set}ting uses an N-frame training snippet S = III. NETWORKDETAILS
I ,I , ,I from a video as input. The FisheyeDis-
1 2 N A. Deformable Super-Resolution Distance and PoseNet
tanceNet can estimate the distance of each image in the
trainingsequence.Anotherconstraintcanbeenforcedamong The distance estimation network is mainly based on the
the frames in S, since the distances of a 3D point estimated U-net architecture [42], an encoder-decoder network with
from different frames should be consistent. skip connections. After testing different variants of ResNet
Let us assume DË† (cid:48) and DË† are the estimates of the family, such as ResNet50 with 25M parameters, we chose
t t âˆˆ
images I (cid:48) and I respectively. For each pixel p I , a ResNet18 [43] as the encoder. The key aspect here is
t t t t
we can use Eq. 7 to obtain pË†(cid:48). Since itâ€™s coordinates are replacingnormalconvolutionswithdeformableconvolutions
t
real valued, we apply the differentiable spatial transformer sinceregularCNNsareinherentlylimitedinmodelinglarge,
network introduced by [28] and estimate the distance value unknown geometric distortions due to their ï¬xed structures,
ofpË†(cid:48) byperformingbilinearinterpolationofthefourpixelâ€™s such as ï¬xed ï¬lter kernels, ï¬xed receptive ï¬eld sizes, and
t
values in DË† (cid:48) which lie close to pË†(cid:48). Let us denote the ï¬xed pooling kernels [44], [24].
t t
distance map obtained through this as DË† â†’ (cid:48)(p ). Next, we In previous works [14], [15], [16], [17], [36], the decoded
t t t
can transform the point cloud in frame I to frame I (cid:48) by features were upsampled via a nearest neighbor interpola-
t t
ï¬rstobtainingP usingEq. 6.Wetransformthepointcloud tion or with learnable transposed convolutions. The main
t
P using the pose networkâ€™s estimate via PË†(cid:48) = T â†’ (cid:48)P . drawback of this process is that it may lead to large errors
t (cid:107) (cid:107) t t t t
Now, D â†’ (cid:48)(p ) := PË†(cid:48) denotes the distance generated at object boundaries in the upsampled distance map as the
t t t t
from point cloud PË†(cid:48). Ideally, D â†’ (cid:48)(p ) and DË† â†’ (cid:48)(p ) interpolationsimplycombinesdistancevaluesofbackground
t t t t t t t
should be equal. Therefore, we can deï¬ne the following andforeground.Foreffectiveanddetailedpreservationofthe
cross-sequence distance consistency loss (CSDCL) for the decoded features, we leverage the concept of sub-pixel con-
training(cid:88)sequ(cid:88)ence(cid:18)S:(cid:88) (cid:12)(cid:12) (cid:12)(cid:12) volutions [25] to our super resolution network. We use pixel
(cid:12) (cid:12) shufï¬‚e convolutions and replace the convolutional feature
âˆ’
Ldc =N 1 N (cid:88)Mtâ†’t(cid:48)(cid:12)(cid:12)Dtâ†’t(cid:48)(pt)âˆ’DË†tâ†’t(cid:48)(pt) (cid:12)(cid:12)(cid:19) uoprsawmitphlinlega,rpnearbfloermtreadnsvpioaseadnecaornevsotlnuetiiognhsb.orTihneterrpeosulalttiionng
(cid:48) (cid:12) (cid:12)
t=1 t=t+1 pt distancemapsaresuper-resolved,havesharpboundariesand
+ M (cid:48)â†’ D (cid:48)â†’ (p (cid:48))âˆ’DË† (cid:48)â†’ (p (cid:48)) expose more details of the scene.
t t t t t t t t
The backbone of our pose estimation network is based
p(cid:48)
t (12) on[14]andpredictsrotationusingEulerangleparameteriza-
tion.TheoutputisasetofsixDOFtransformationsbetween
Eq. 12 contains one term for which pixe(cid:48)ls and point clouds Itâˆ’1 andIt aswellasIt andIt+1.Wehavereplacednormal
are warped forwards in time (from t to t) and one term for convolutions with deformable convolutions for the encoder-
(cid:48)
which they are warped backwards in time (from t to t). decoder setting.
In prior works [32], [37], the consistency error is limited
B. Implementation Details
toonlytwoframes,whereasweapplyittotheentiretraining
sequence S. This induces more constraints and enlarges the We use Pytorch [45] and employ Adam [46] optimizer to
baseline, inherently improving the distance estimation [27]. minimize the training objective function (13) with Î² =0.9,
1
577
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. Abs Rel Sq Rel RMSE RMSE (log) Î´ <1.25 Î´ <1.252 Î´ <1.253
Approach lower is better higher is better
KITTI
Zhou [15]â€  0.183 1.595 6.709 0.270 0.734 0.902 0.959
Yang [38] 0.182 1.481 6.501 0.267 0.725 0.906 0.963
Vid2depth [35] 0.163 1.240 6.220 0.250 0.762 0.916 0.968
GeoNet [36]â€  0.149 1.060 5.567 0.226 0.796 0.935 0.975
DDVO [17] 0.151 1.257 5.583 0.228 0.810 0.936 0.974
DF-Net [37] 0.150 1.124 5.507 0.223 0.806 0.933 0.973
Ranjan [39] 0.148 1.149 5.464 0.226 0.815 0.935 0.973
EPC++ [33] 0.141 1.029 5.350 0.216 0.816 0.941 0.976
Struct2depth â€˜(M)â€™ [34] 0.141 1.026 5.291 0.215 0.816 0.945 0.979
Zhou [27] 0.139 1.057 5.213 0.214 0.831 0.940 0.975
PackNet-SfM [40] 0.120 0.892 4.898 0.196 0.864 0.954 0.980
Monodepth2 [14] 0.115 0.903 4.863 0.193 0.877 0.959 0.981
FisheyeDistanceNet 0.117 0.867 4.739 0.190 0.869 0.960 0.982
Ã—
FisheyeDistanceNet (1024 320) 0.109 0.788 4.669 0.185 0.889 0.964 0.982
WoodScape
FisheyeDistanceNet cap 80m 0.167 1.108 3.814 0.216 0.794 0.953 0.972
FisheyeDistanceNet cap 40m 0.152 0.768 2.723 0.210 0.812 0.954 0.974
FisheyeDistanceNet cap 30m 0.149 0.613 2.402 0.204 0.810 0.957 0.976
TABLEIQuantitativeresultsofleaderboardalgorithmsonKITTIdataset[23]andFisheyeDistanceNetonFisheyedatasetpartofWoodScape[1].
Single-viewdepthestimationresultsusingtheEigenSplit[41]fordepthsreportedlessthan80m,asindicatedin[41]forpinholemodel.Allthe
approachesareself-supervisedonmonocularvideosequences.Attest-time,allmonocularmethodsexcludingourFisheyeDistanceNet,scaletheestimated
depthsusingmedianground-truthLiDARdepth.Fortheï¬sheyedataset,weestimatedistanceratherthandepth.â€ marksnewerresultsreportedonGitHub.
Î² =0.999. We train the model for 25 epochs, with a batch distance maps using sub-pixel convolution [25], we initial-
2
size of 20 on 24GB Titan RTX with initial learning rate of ized the last convolutional layer in a speciï¬c way before the
âˆ’ âˆ’
10 4 fortheï¬rst20epochs,thendropto10 5 forthelast5 pixel shufï¬‚e operation as described in [50].
epochs. The sigmoided output Ïƒ from the distance decoder
Â·
is converted to distance with D =a Ïƒ+b. For the pinhole IV. EXPERIMENTS
Â·
model, depth D = 1/(a Ïƒ+b), where a and b are chosen A. Datasets
toconstrainD between0.1and100units.Theoriginalinput
Ã— 1) WoodScape â€“ Fisheye Dataset: The dataset contains
resolutionoftheï¬sheyeimageis1280 800pixels,wecrop
Ã— roughly 40,000 raw images obtained with a ï¬sheye camera
itto1024 512toremovethevehicleâ€™sbumper,shadowand
and point clouds from a sparse Velodyne HDL-64E rotating
other artifacts of the vehicle. Finally the cropped image is
Ã— 3D laser scanner as ground truth for the test set. The
downscaled to 512 256 before feeding to the network. For
Ã— training set contains 39,038 images collected by driving
pinhole model on KITTI, we use 640 192 pixels as the
around various parts of Bavaria, Germany. The validation
network input.
and the test split contain 1,214 and 697 images respectively.
Weexperimentedwithbatchnormalization[47]andgroup
The dataset distribution is similar to the KITTI Eigen split
normalization[48]layersintheencoder-decodersetting.We
used in [14], [15] for the pinhole model. The training set
have found that group normalization with G = 32 signiï¬- comprises three scene categories: city, residential and sub-
cantlyimprovestheresults[49].Thesmoothnessweightterm
urban.Whiletraining,thesecategoriesarerandomlyshufï¬‚ed
Î² and cross-sequence distance consistency weight term Î³ and fed to the network. We ï¬lter static scenes based on the
have been set to 0.001. We applied deformable convolutions speed of the vehicle with a threshold of 2km/h to remove
tothe3x3convlayersinstagesconv3,conv4,andconv5in
imageframesthatonlyobserveminimalcameraego-motion,
ResNet18 and ResNet50, with 12 layers of deformable con-
since distance cannot be learned under these circumstances.
volutionintheencoderpartcomparedto3layersin[44],all
Comparable to previous experiments on pinhole SfM [15],
intheconv5stageforResNet50.Wereplacedthesubsequent
[14], we set the length of the training sequence to 3.
layers of the decoder with deformable convolutions for the
2) KITTI â€“ Eigen Split: We use the KITTI dataset and
distanceandposenetwork.Forthepinholemodel,onKITTI
data split according to Eigen et al. [51] for the experiments
Eigen split in Section IV-A.2 we used normal convolutions
with pinhole image data. We ï¬lter static frames as proposed
instead of deformable convolutions.
byZhouetal.[15].Theresultingtrainingsetcontains39,810
Finally,toalleviatecheckerboardartifactsfromtheoutput images and the validation split comprises 4,424 images. We
578
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. ut
p
n
I
w
a
R
ut
p
n
I
d
e
p
p
o
r
C
Fig.3QualitativeresultsontheFisheyeWoodScapedataset.OurFisheyeDistanceNetproducessharpdistancemapsonrawï¬sheyeimages.
Method FS BS SR CSDCL DCN Abs Rel Sq Rel RMSE RMSE log Î´<1.25 Î´<1.252 Î´<1.253
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
Ours 0.152 0.768 2.723 0.210 0.812 0.954 0.974
(cid:88) (cid:88) (cid:88) (cid:88)
Ours 0.172 0.829 2.925 0.243 0.802 0.952 0.970
(cid:88) (cid:88) (cid:88)
Ours 0.181 0.913 3.180 0.250 0.823 0.938 0.963
(cid:88) (cid:88)
Ours 0.190 0.997 3.266 0.258 0.796 0.930 0.963
(cid:88)
Ours 0.201 1.282 3.589 0.276 0.590 0.898 0.949
TABLEIIAblationstudyondifferentvariantsofourFisheyeDistanceNetusingtheFisheyeWoodScapedataset[1].Distancesarecappedat40m.BS,
SR,CSDCLandDCNrepresentbackwardsequence,super-resolutionnetworkwithPixelShufï¬‚eorsub-pixelconvolutioninitializedtoconvolutionNN
Ã—
resize(ICNR)[50],cross-sequencedistanceconsistencylossanddeformableconvolutionsrespectively.Theinputresolutionis512 256pixels.
use the standard test set of 697 images. The length of the is only trained for the forward sequence which consists
training sequence is set to 3. of two warps as explained in Section II-E; (ii) Addition-
ally remove Super Resolution using sub-pixel convolution:
B. Evaluation
Removal of sub-pixel convolution has a huge impact on
WeevaluateFisheyeDistanceNetâ€™sdepthanddistanceesti-
Woodscape compared to KITTI. This is mainly attributed
mationresultsusingthemetricsproposedbyEigenetal.[41]
totheï¬sheyemodel,asfar-awayobjectsaretinyandcannot
to facilitate comparison. The quantitative results shown in
be resolved accurately with naive nearest neighbor inter-
the Table I illustrate that our scale-aware self-supervised
polation or transposed convolution [26]; (iii) Additionally
approach outperforms all the state-of-the-art monocular ap-
remove cross-sequence distance consistency loss: Removing
proaches. We could not leverage the Cityscapes dataset into
theCSDCLmainlydiminishesthebaseline;(iv)Additionally
our training regime to benchmark our scale-aware frame-
remove deformable convolutions: If we remove all the major
work, due to absence of odometry data.
components, especially deformable convolution layers [24],
Since the projection operators are different, previous SfM
our model will fail miserably as the distortion introduced
approaches will not be feasible on Fisheye Woodscape
by ï¬sheye model will not be learned correctly by normal
dataset without adaption of the network and projection
convolutional layers.
model. It is important to note that due to the geometry of
the ï¬sheye, it would not be a fair comparison to evaluate V. CONCLUSION
We propose a novel self-supervised training strategy to
the distance estimates up to 80m. Our ï¬sheye automotive
obtain metric distance maps on unrectiï¬ed ï¬sheye im-
camerasalsoundergohighdatacompressionandourdataset
ages. Through extensive experiments, we show that our
contains images of inferior quality when compared with
FisheyeDistanceNetestablishesanewstateoftheartinself-
KITTI. Our ï¬sheye cameras can perform well up to a range
supervisedmonoculardistanceanddepthestimationonFish-
of 40m. Therefore, we also report results on a 30m and a
eye WoodScape and KITTI dataset respectively. We obtain
40m. range (see Table I).
promisingresultsdemonstratingthepotentialofusingaCNN
C. Fisheye Ablation Study
based approach for deployment in commercial automotive
Weconductanablationstudytoevaluatetheimportanceof systems, in particular for replacing current classical depth
different components. We ablate the following components estimation approaches. To encourage further research on
and report their impact on the distance evaluation metrics ï¬sheye distance estimation, we will release the dataset as
in Table II: (i) Remove Backward Sequence: The network a part of WoodScape [1] project.
579
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [21] S. Li and K. Fukumori, â€œSpherical stereo for the construction of
immersive vr environment,â€ in IEEE Proceedings. VR 2005. Virtual
[1] S. Yogamani, C. Hughes, J. Horgan, G. Sistu, P. Varley, D. Oâ€™Dea, Reality,2005. IEEE,2005,pp.217â€“222.
M. UricaÂ´r, S. Milz, M. Simon, K. Amende, et al., â€œWoodscape:
[22] J. Huang, Z. Chen, D. Ceylan, and H. Jin, â€œ6-dof vr videos with a
A multi-task, multi-camera ï¬sheye dataset for autonomous driving,â€
single360-camera,â€in2017IEEEVirtualReality(VR). IEEE,2017,
in Proceedings of the IEEE International Conference on Computer
pp.37â€“44.
Vision,2019,pp.9308â€“9318.
[23] A.Geiger,P.Lenz,C.Stiller,andR.Urtasun,â€œVisionmeetsrobotics:
[2] M.Heimberger,J.Horgan,C.Hughes,J.McDonald,andS.Yogamani,
The kitti dataset,â€ The International Journal of Robotics Research,
â€œComputer vision in automated parking systems: Design, implemen-
vol.32,no.11,pp.1231â€“1237,2013.
tationandchallenges,â€ImageandVisionComputing,vol.68,pp.88â€“
[24] X. Zhu, H. Hu, S. Lin, and J. Dai, â€œDeformable convnets v2: More
101,2017.
deformable,betterresults,â€inProceedingsoftheIEEEConferenceon
[3] A.Dahal,J.Hossen,C.Sumanth,G.Sistu,K.Malhan,M.Amasha,
ComputerVisionandPatternRecognition,2019,pp.9308â€“9316.
and S. Yogamani, â€œDeeptrailerassist: Deep learning based trailer de-
[25] W. Shi, J. Caballero, F. HuszaÂ´r, J. Totz, A. P. Aitken, R. Bishop,
tection,trackingandarticulationangleestimationonautomotiverear-
D.Rueckert,andZ.Wang,â€œReal-timesingleimageandvideosuper-
view camera,â€ in Proceedings of the IEEE International Conference
resolutionusinganefï¬cientsub-pixelconvolutionalneuralnetwork,â€
onComputerVisionWorkshops,2019,pp.0â€“0.
inProceedingsoftheIEEEconferenceoncomputervisionandpattern
[4] J.Horgan,C.Hughes,J.McDonald,andS.Yogamani,â€œVision-based
recognition,2016,pp.1874â€“1883.
driver assistance systems: Survey, taxonomy and advances,â€ in 2015
IEEE 18th International Conference on Intelligent Transportation [26] A. Odena, V. Dumoulin, and C. Olah, â€œDeconvolution and checker-
Systems. IEEE,2015,pp.2032â€“2039. boardartifacts,â€Distill,vol.1,no.10,p.e3,2016.
[5] M.Drulea,I.Szakats,A.Vatavu,andS.Nedevschi,â€œOmnidirectional [27] L. Zhou, J. Ye, M. Abello, S. Wang, and M. Kaess, â€œUnsupervised
stereo vision using ï¬sheye lenses,â€ in 2014 IEEE 10th International learningofmonoculardepthestimationwithbundleadjustment,super-
Conference on Intelligent Computer Communication and Processing resolutionandcliploss,â€arXivpreprintarXiv:1812.03368,2018.
(ICCP). IEEE,2014,pp.251â€“258. [28] M.Jaderberg,K.Simonyan,A.Zisserman,etal.,â€œSpatialtransformer
[6] D. Caruso, J. Engel, and D. Cremers, â€œLarge-scale direct slam for networks,â€ in Advances in neural information processing systems,
omnidirectionalcameras,â€in2015IEEE/RSJInternationalConference 2015,pp.2017â€“2025.
onIntelligentRobotsandSystems(IROS). IEEE,2015,pp.141â€“148. [29] H.Zhao,O.Gallo,I.Frosio,andJ.Kautz,â€œLossfunctionsforimage
[7] G.Sistu,I.Leang,andS.Yogamani,â€œReal-timejointobjectdetection restoration with neural networks,â€ IEEE Transactions on Computa-
and semantic segmentation network for automated driving,â€ arXiv tionalImaging,vol.3,no.1,pp.47â€“57,2016.
preprintarXiv:1901.03912,2019. [30] Z.Wang,A.C.Bovik,H.R.Sheikh,E.P.Simoncelli,etal.,â€œImage
[8] M.UË‡ricË‡aÂ´Ë‡r,P.KË‡rÂ´Ä±zË‡ek,G.Sistu,andS.Yogamani,â€œSoilingnet:Soiling qualityassessment:fromerrorvisibilitytostructuralsimilarity,â€IEEE
detectiononautomotivesurround-viewcameras,â€in2019IEEEIntel- transactionsonimageprocessing,vol.13,no.4,pp.600â€“612,2004.
ligent Transportation Systems Conference (ITSC). IEEE, 2019, pp. [31] Z.ArÄ±canandP.Frossard,â€œDensedepthestimationfromomnidirec-
67â€“72. tionalimages,â€2009.
[9] M.Yahiaoui,H.Rashed,L.Mariotti,G.Sistu,I.Clancy,L.Yahiaoui, [32] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and
V. R. Kumar, and S. Yogamani, â€œFisheyemodnet: Moving object K. Fragkiadaki, â€œSfm-net: Learning of structure and motion from
detection on surround-view cameras for autonomous driving,â€ arXiv video,â€arXivpreprintarXiv:1704.07804,2017.
preprintarXiv:1908.11789,2019. [33] C.Luo,Z.Yang,P.Wang,Y.Wang,W.Xu,R.Nevatia,andA.Yuille,
[10] M. UricaÂ´r, J. Ulicny, G. Sistu, H. Rashed, P. Krizek, D. Hurych, â€œEvery pixel counts++: Joint learning of geometry and motion with
A. Vobecky, and S. Yogamani, â€œDesoiling dataset: Restoring soiled 3dholisticunderstanding,â€arXivpreprintarXiv:1810.06125,2018.
areas on automotive ï¬sheye cameras,â€ in Proceedings of the IEEE [34] V.Casser,S.Pirk,R.Mahjourian,andA.Angelova,â€œDepthprediction
International Conference on Computer Vision Workshops, 2019, pp. without the sensors: Leveraging structure for unsupervised learning
0â€“0. from monocular videos,â€ in Proceedings of the AAAI Conference on
[11] N. Tripathi, G. Sistu, and S. Yogamani, â€œTrained trajectory based Artiï¬cialIntelligence,vol.33,2019,pp.8001â€“8008.
automated parking system using visual slam,â€ arXiv preprint
[35] R.Mahjourian,M.Wicke,andA.Angelova,â€œUnsupervisedlearning
arXiv:2001.02161,2020.
of depth and ego-motion from monocular video using 3d geometric
[12] V. R. Kumar, S. Milz, C. Witt, M. Simon, K. Amende, J. Petzold,
constraints,â€ in Proceedings of the IEEE Conference on Computer
S. Yogamani, and T. Pech, â€œMonocular ï¬sheye camera depth esti-
VisionandPatternRecognition,2018,pp.5667â€“5675.
mation using sparse lidar supervision,â€ in 2018 21st International
[36] Z. Yin and J. Shi, â€œGeonet: Unsupervised learning of dense depth,
Conference on Intelligent Transportation Systems (ITSC). IEEE,
opticalï¬‚owandcamerapose,â€inProceedingsoftheIEEEConference
2018,pp.2853â€“2858.
onComputerVisionandPatternRecognition,2018,pp.1983â€“1992.
[13] N. Zioulis, A. Karakottas, D. Zarpalas, and P. Daras, â€œOmnidepth:
[37] Y.Zou,Z.Luo,andJ.-B.Huang,â€œDf-net:Unsupervisedjointlearning
Densedepthestimationforindoorssphericalpanoramas,â€inProceed-
ofdepthandï¬‚owusingcross-taskconsistency,â€inProceedingsofthe
ingsoftheEuropeanConferenceonComputerVision(ECCV),2018,
EuropeanConferenceonComputerVision(ECCV),2018,pp.36â€“53.
pp.448â€“465.
[38] Z. Yang, P. Wang, W. Xu, L. Zhao, and R. Nevatia, â€œUnsupervised
[14] C. Godard, O. Mac Aodha, M. Firman, and G. Brostow, â€œDig-
learningofgeometryfromvideoswithedge-awaredepth-normalcon-
gingintoself-supervisedmonoculardepthestimation,â€arXivpreprint
sistency,â€inThirty-SecondAAAIConferenceonArtiï¬cialIntelligence,
arXiv:1806.01260,2018.
2018.
[15] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, â€œUnsupervised
learningofdepthandego-motionfromvideo,â€inProceedingsofthe [39] A. Ranjan, V. Jampani, L. Balles, K. Kim, D. Sun, J. Wulff, and
IEEEConferenceonComputerVisionandPatternRecognition,2017, M. J. Black, â€œCompetitive collaboration: Joint unsupervised learning
pp.1851â€“1858. of depth, camera motion, optical ï¬‚ow and motion segmentation,â€ in
[16] C.Godard,O.MacAodha,andG.J.Brostow,â€œUnsupervisedmonoc- ProceedingsoftheIEEEConferenceonComputerVisionandPattern
ulardepthestimationwithleft-rightconsistency,â€inCVPR,2017. Recognition,2019,pp.12240â€“12249.
[17] C. Wang, J. Miguel Buenaposada, R. Zhu, and S. Lucey, â€œLearning [40] V. Guizilini, R. Ambrus, S. Pillai, and A. Gaidon, â€œPacknet-sfm:
depth from monocular videos using direct methods,â€ in The IEEE 3d packing for self-supervised monocular depth estimation,â€ arXiv
ConferenceonComputerVisionandPatternRecognition(CVPR),June preprintarXiv:1905.02693,2019.
2018. [41] D. Eigen, C. Puhrsch, and R. Fergus, â€œDepth map prediction from
[18] S. Li, â€œBinocular spherical stereo,â€ IEEE Transactions on intelligent a single image using a multi-scale deep network,â€ CoRR, vol.
transportationsystems,vol.9,no.4,pp.589â€“600,2008. abs/1406.2283, 2014. [Online]. Available: http://arxiv.org/abs/1406.
[19] C.Ma,L.Shi,H.Huang,andM.Yan,â€œ3dreconstructionfromfull- 2283
viewï¬sheyecamera,â€arXivpreprintarXiv:1506.06273,2015. [42] O. Ronneberger, P. Fischer, and T. Brox, â€œU-net: Convolutional
[20] S.Pathak,A.Moro,A.Yamashita,andH.Asama,â€œDense3drecon- networksforbiomedicalimagesegmentation,â€inInternationalConfer-
struction from two spherical images via optical ï¬‚ow-based equirect- enceonMedicalimagecomputingandcomputer-assistedintervention.
angularepipolarrectiï¬cation,â€in2016IEEEInternationalConference Springer,2015,pp.234â€“241.
onImagingSystemsandTechniques(IST). IEEE,2016,pp.140â€“145. [43] K.He,X.Zhang,S.Ren,andJ.Sun,â€œDeepresiduallearningforimage
580
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. recognition,â€ in Proceedings of the IEEE conference on computer
visionandpatternrecognition,2016,pp.770â€“778.
[44] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,
â€œDeformable convolutional networks,â€ in Proceedings of the IEEE
internationalconferenceoncomputervision,2017,pp.764â€“773.
[45] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito,
Z.Lin,A.Desmaison,L.Antiga,andA.Lerer,â€œAutomaticdifferen-
tiationinPyTorch,â€inNIPSAutodiffWorkshop,2017.
[46] D. P. Kingma and J. Ba, â€œAdam: A method for stochastic optimiza-
tion,â€arXivpreprintarXiv:1412.6980,2014.
[47] S. Ioffe and C. Szegedy, â€œBatch normalization: Accelerating deep
networktrainingbyreducinginternalcovariateshift,â€inProceedings
of the 32Nd International Conference on International Conference
onMachineLearning-Volume37,ser.ICMLâ€™15. JMLR.org,2015,
pp. 448â€“456. [Online]. Available: http://dl.acm.org/citation.cfm?id=
3045118.3045167
[48] Y. Wu and K. He, â€œGroup normalization,â€ in Proceedings of the
EuropeanConferenceonComputerVision(ECCV),2018,pp.3â€“19.
[49] K.He,R.Girshick,andP.DollaÂ´r,â€œRethinkingimagenetpre-training,â€
arXivpreprintarXiv:1811.08883,2018.
[50] A. Aitken, C. Ledig, L. Theis, J. Caballero, Z. Wang, and W. Shi,
â€œCheckerboardartifactfreesub-pixelconvolution:Anoteonsub-pixel
convolution,resizeconvolutionandconvolutionresize,â€arXivpreprint
arXiv:1707.02937,2017.
[51] D.EigenandR.Fergus,â€œPredictingdepth,surfacenormalsandseman-
tic labels with a common multi-scale convolutional architecture,â€ in
ProceedingsoftheIEEEinternationalconferenceoncomputervision,
2015,pp.2650â€“2658.
581
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. 