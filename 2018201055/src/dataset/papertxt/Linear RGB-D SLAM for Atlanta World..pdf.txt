2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Linear RGB-D SLAM for Atlanta World
Kyungdon Joo1, Tae-Hyun Oh2, Francois Rameau3, Jean-Charles Bazin3 and In So Kweon3
Abstractâ€”WepresentanewlinearmethodforRGB-Dbased Perspective view
simultaneous localization and mapping (SLAM). Compared to
existingtechniquesrelyingontheManhattanworldassumption
deï¬nedbythreeorthogonaldirections,ourapproachisdesigned
forthemoregeneralscenariooftheAtlantaworld.Itconsistsof
averticaldirectionandasetofhorizontaldirectionsorthogonal Top-down view
to the vertical direction and thus can represent a wider range
of scenes. Our approach leverages the structural regularity of
theAtlantaworldtodecouplethenon-linearityofcamerapose
estimations. This allows us separately to estimate the camera Fig.1:Atlantaframe-awareLinearSLAMontheTAMURGB-
rotation and then the translation, which bypasses the inherent D dataset [10]. Left: The estimated Atlanta structure (ï¬ve Atlanta
non-linearity of traditional SLAM techniques. To this end, we directions in total) by the proposed tracking-by-detection scheme,
introduce a novel tracking-by-detection scheme to estimate the where the red arrow denotes the vertical direction and the others
underlyingscenestructurebyAtlantarepresentation.Thereby, indicate the observed horizontal directions. Right: The estimated
we propose an Atlanta frame-aware linear SLAM framework camera trajectory and Atlanta planar map, where we visualize
which jointly estimates the camera motion and a planar map the planar features supporting the Atlanta structure of the scene;
supportingtheAtlantastructurethroughalinearKalmanï¬lter. these planes are also color-overlaid on the sampled images. For
Evaluations on both synthetic and real datasets demonstrate visualization purposes, we display the sampled camera trajectory,
thatourapproachprovidesfavorableperformancecomparedto omittheplanarmapssupportingtheverticaldirection,androughly
existingstate-of-the-artmethodswhileextendingtheirworking obtain the plane boundaries as a pseudo representation.
range to the Atlanta world.
as the Manhattan frame (MF). Under the MW assumption,
I. INTRODUCTION
Kim et al. estimate the camera rotation by tracking a single
Visual simultaneous localization and mapping (visual
MF.Thisapproachallowsdecouplingoftherotationestima-
SLAM) is a technique which jointly estimates the 6-DoF
tion such that the camera position and the planesâ€™ compo-
camera motion and 3D structure of an unknown environ-
sition of the MF can be jointly estimated through a linear
ment. Visual SLAM methods have been widely studied in
Kalman ï¬lter (KF). L-SLAM demonstrates its effectiveness
the robotics and computer vision communities for several
under texture-less environments and has low computational
decades [1]. They have demonstrated promising results and
complexity owing to the linear formulation of the problem.
have been applied to various vision applications, such as 3D
However, the strict MW assumption of L-SLAM limits its
modeling, augmented reality (AR), and autonomous driving.
application range to cuboid-shaped structures.
However, they remain sensitive to challenging conditions
ToalleviatethelimitationsofMW,weextendtheprevious
such as texture-less or feature-less scenes typical in man-
L-SLAM approach to a more general structural represen-
made environments [2], [3] (such as indoor scenes). To
tation called the Atlanta world (AW) assumption [12], in
cope with such limitations, a common alternative is to
which multiple horizontal directions are orthogonal to the
take advantage of additional high-level primitives such as
vertical direction but, contrary to the MW, do not have to
planarfeaturesinstructuredenvironments[4],[5],[6].These
be orthogonal to each other (e.g., walls in a room are not
extrafeaturesimprovetherobustnessagainstpoorlytextured
perpendicular). As a result, the AW can express a wider
scenes, especially in the case of graph-based SLAM meth-
range of scenes [12], [13], [14]. However, compared to the
ods[7],[8].However,theapplicabilityofthesetechniquesis
MW and its two horizontal directions which are orthogonal
limitedbytheirhighcomplexity,astheresultinggraph-based
to each other, the AW assumption does not impose any
optimization is non-convex and non-linear [9].
pre-determined number of horizontal directions or any prior
To overcome these limitations, Kim et al. [3] proposed a
relative orientations of these directions. Thus, estimating the
linearRGB-DSLAMmethod(L-SLAM)whichreliesonthe
Atlanta structure, referred to as the Atlanta frame (AF), of a
structural simplicity of a class of man-made environments,
scene itself is a challenging and non-trivial task.
the Manhattan world (MW) [11]. MW is deï¬ned by three
In this work, we introduce a novel tracking-by-detection
orthogonal directions, and these directions are referred to
scheme to estimate the AF from RGB-D data. The tracking
algorithm is complemented by a detection module that iden-
1K.JooiswiththeRoboticsInstitute,CarnegieMellonUniversity(CMU
RI),Pittsburgh,US.kjoo@andrew.cmu.edu tiï¬es new or missing directions and improves the robustness
2T.-H. Oh is with Dept. of EE, POSTECH, Pohang, South Korea. ofthetracking.Inaddition,bydeployingAW(theestimated
taehyun@postech.ac.kr
AF), we propose an Atlanta frame-aware linear SLAM
3F. Rameau, J.-C. Bazin and I. S. Kweon are with KAIST, Daejeon,
SouthKorea.{frameau, bazinjc, iskweon77}@kaist.ac.kr framework, referredto as AF-SLAM, whichjointly estimates
978-1-7281-7395-5/20/$31.00 Â©2020 IEEE 1077
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:24:53 UTC from IEEE Xplore.  Restrictions apply. the camera motion and planar map supporting the Atlanta (a) (b) (c) (d)
ğ«1
structure (see Fig. 1). In summary, the contributions of this ğ«3
work are as follows: O
â€¢ We introduce a novel tracking-by-detection scheme for ğ«2
Atlanta world to estimate the underlying unknown At- ğ¯ğ‘£=ğ«1
lanta structure of a scene and the camera rotation.
â€¢ WeproposealinearSLAMmethodbasedontheAtlanta ğ¯â„1=ğ«Î±22 Î±ğ¯mâ„2ğ¯â„ğ‘š
frame (AF-SLAM) that localizes the camera position
Fig. 2: Manhattan vs. Atlanta representation: (a) MF (top)
and generates a set of planar maps supporting the represented by a rotation matrix and AF (bottom) modeled by a
underlying Atlanta structure. rotation matrix and an angle set [13]. (b) Examples of 3D point
cloudsfromtheNYUv2RGB-Ddataset[26],wheretheblackboxes
In addition, we evaluate the proposed AF-SLAM on
show top-down views of the 3D point clouds. (c) The MFs on the
synthetic and real-world RGB-D benchmark datasets.
input normal distributions and the corresponding segmentations,
where the estimated solutions do not cover the scenes well [27].
II. RELATEDWORK
On the other hand, (d) the AFs estimated by the aforementioned
In this section, we brieï¬‚y review SLAM approaches study [13] represent nearly all of the scenes.
that utilize high-level geometric features such as planes to
cope with texture-less man-made environments. For general visual-inertial odometry. These approaches either depend on
SLAM methods, readers can refer to ealier work [1]. the MW assumption or utilize nonlinear optimization.
Planar features have been exploited as additional fea- The work most related to our approach is L-SLAM [3].
ture within an Extended Kalman Filter (EKF)-based SLAM Following on the de-coupled motion demonstrated in their
framework [15]. Gee et al. [16] integrated point features previousworks[23],[24],theyexploitedplanarfeatures(1D
lying on the same plane into a planar feature in a bottom-up distance w.r.t. planes supporting the MW structure) as mea-
manner, which lessens the burden in EKF-SLAM. MartÂ´Ä±nez- surement targets and formulated a linear KF-based SLAM
Carranza and Calway [17] introduced a uniï¬ed parametriza- framework with low complexity. However, L-SLAM only
tion for both planar and point features in the EKF-SLAM works for the MW structure, and even a single MF tracking
framework. In addition, graph-based SLAM [7], which is a failure in L-SLAM will break the SLAM pipeline. On the
main stream in SLAM research, has utilized planar features other hand, our approach, AF-SLAM, covers more general
within their framework. For instance, Kaess [18] introduced structuralenvironmentsthatfollowtheAWassumptionwhile
a minimal representation for planar features and formulated maintaining linear computational complexity.
the representation in pose graph optimization. Ma et al. [19]
deï¬ned a global plane model and tracked it to the esti- III. TRACKING-BY-DETECTIONALGORITHMFORTHE
mate camera pose based on direct image alignment and ATLANTAFRAME
graph optimization. Yang et al. [6] proposed a graph-based Our linear SLAM framework leverages the Atlanta frame
SLAM framework combined with a planar scene layout; in (AF) of a given scene, in which the Kalman ï¬lter updates
this framework, plane landmark measurements come from states by relying on the estimated AF. In this section, we
a single-image based pop-up model. However, the planar deï¬ne the AF ï¬rst and then propose an AF tracking-and-
SLAM approaches described above are associated with high detection algorithm, which provides a robust AF estimation
computational complexity caused by the rapidly growing forthesubsequentKalmanï¬lter.Weutilizebothsurfacenor-
number of variables in the case of EKF-based SLAM, or mals(fromdepth)andlinenormals(fromcalibratedimages)
they are expensive and difï¬cult pose-graph optimization as input data for tracking-by-detection. For simplicity, here
processes that are necessary to solve nonlinear and non- weexplainthedetailsforthesurfacenormalsbutthisprocess
convex optimization problems. is seamlessly applicable to both types of normals.
Ingeneral,planarfeaturesinman-madeenvironmentshave
A. Atlanta frame representation
structural forms whose properties can be utilized as a priori
information or structural regularities in SLAM, registration, We represent an Atlanta frame as a set of unit vec-
V { Â·Â·Â· } { }
and navigation [2], [20], [21]. Struab et al. [22] utilized the tors = v ,v ,v , ,v = v M+1 that consists of
v h1 h2 hM m m=1
MW structure to estimate rotation robust against angular a vertical direction v =v and M horizontal direc-
v 1
drift. Kim et al. [23], [24] tracked the Manhattan frame tions v =v orthogonal to the vertical direction, i.e.,
âŠ¥ hm m+1 âˆˆ{ Â·Â·Â· }
of a given scene to estimate drift-free rotational motion, v v (called the AW constraint) for m 1, ,M . In
v hm V
which enables the de-coupling of translational motion. Le the rest of this paper, we refer to this direction set as the
and KosË‡ecka [5] proposed a planar RGB-D SLAM method Atlanta frame (AF) or Atlanta directions.
that estimates the camera rotation by identifying local MFs The AW constraint is the only constraint required for
between subsequent frames and then infers 2-DoF camera the AF, which is far more general than the Manhattan
translation in a graph SLAM framework. Recently, the frame (MF) but sufï¬ciently constrains the parameter space
Atlanta world has begun to attract attention. Li et al. [21] such that it enables a robust and efï¬cient AF search of
leveraged the structural regularity of AW for a monocular a scene. The MF uses only three orthogonal directions to
SLAM. Zou et al. [25] exploited the AW assumption for represent the structure of the scene, which can simply be
1078
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:24:53 UTC from IEEE Xplore.  Restrictions apply. (a) Input (b)Mean shift-based tracking (d)Association (e)Output
Local association Global association
Mean-shift
: Initial position
: Estimated position Tracked AF ğ“¥ğ‘˜ğ‘‡ Local AF ğ“¥ğ¿ğ‘˜ Updated local AF ğ“¥ğ¿ğ‘˜
(c)BnB-based detection
Branch-and-bound
Surface normal 
with previous AF ğ“¥ğ¿ğ‘˜âˆ’1 ğ›¼4
Detected AF ğ“¥ğ·ğ‘˜ Global AF ğ“¥ğº Updated global AF ğ“¥ğº
Fig. 3: Overview of the tracking-by-detection scheme. (a) Given surface normal distribution at the k-th frame with the previous local
V âˆ’
AF k 1 (dashed arrows), we independently perform (b) mean shift-based AF tracking and (c) BnB-based AF detection. (d) We then
L V V V
apply the association step; we associate tracked AF k and detected AF k into the local AF k (local association), where the orange
T D L
arrows indicate the associated directions and the black arrow denotes a potential direction. In the global association, we associate the
V V
local AF k with the global AF , where the new horizontal direction is born (cyan).
L G
represented by a rotation matrix. However, if the scene does Atlanta frame tracking. Zhou et al. [28] formulated the
notfollowthisassumption(e.g.,ifthewallsinaroomarenot dominant axes estimation problem as a problem of mode
perpendicular), this model fails to express the scene. On the seeking in a density distribution, which shows accurate
other hand, the AF can encapsulate an arbitrary number of and efï¬cient performance during the tracking of structural
horizontal directions depending on the observed scene, and regularities such as MF [23]. Similarly to this idea, we
these directions do not need to be orthogonal to each other track each Atlanta direction via a mean shift algorithm in
(see Fig. 2). This strategy makes the model more versatile the tangent plane with a Gaussian kernel. Given the surface
(e.g., a hexagonal room). Therefore, our AF-based method normals distributed on a unit sphere and an initial Atlanta
âˆˆ V âˆ’
can extend the working range of the algorithm compared to direction vl k 1, we project the surface normals into
L L
that in Kim et al. [3], which is limited to the MF. thetangentplaneatvl andinferthemeanofthedirectional
L
Atlanta frame parameterization. We follow the efï¬cient vector distribution around the projection of each vl , as
L
depicted in Fig. 3(b). After applying a mean shift [29], we
AF parametrization proposed by Joo et al. [13]. Their AF
representatiâˆˆon leverages the rotation matrix structure R = emsetiamnaetestitmheattreaocknetodtAhetlaunntiatdspirheecrteio(nfovrTtmboyretrdaentsafiolsrm,rienfgerthtoe
[r ,r ,r ] SO(3) to represent the vertical direction and
1 2 3
the ï¬rst horizontal direction by r1 and r2, i.e., vv=r1 and [28]).IfvTt doesnotcoveragivennormaldistributionwitha
certainpercentageofthetotalnumberofnormals,wediscard
vaadnhdg1il=teiorp2na,arlawmhhoeertrieezrovÎ±nhta1lbadcyitrserocattsaiotainnrgvehfvemrecnabcnyebtlheoecdaaetnï¬iognnlee.dTÎ±ahseana,srioenuagncldhe twhhiserdeirNecTtioisn.thWeenudmenboetreotfhetratrcakcekdedAtAlaFntaasdVirTkec=ti{ovnTts.}Nt=T1,
m h1 m V
the axis v (see Fig. 2(a)). Thus, we represent an AF by Robust Atlanta frame detection. We run an AF detection
{ { }v }
R, Î± M , which includes the AW constraint without processseparatelyfromthetracking(seeFig.3(c)).Through
m m=2
explicitconstraintsandallowsustoformulatealinearSLAM the detection step, we can revive lost Atlanta directions or
adaptedtotheAWassumption(asdescribedinSec.IV).This create new Atlanta directions. This strategy results in more
willbeinterchangeablyusedwithadirectionalrepresentation stabletrackingandenablessustainablelong-termtracks[30].
{ Â·Â·Â· } V
v ,v ,v , ,v in upcoming contexts. Giventhesurfacenormaldistribution,wedetecttheAF k=
v h1 h2 hM { } D
vd ND that best describes the current normal distribution
B. Tracking-by-detection D d=1
by means of the number of inliers. For this purpose, we
Given an RGB-D sequence, we propose a new tracking- utilizeabranch-and-boundbasedAFestimationmethod[13]
by-detectionframeworkthatestimatestheunderlyingAtlanta thatrobustlyestimatestheAFeveninahigh-noisesituation.
V
structure of a scene, called global AF (see Fig. 3). The For the sake of computational efï¬ciency, we detect the AF
G
globalAFcontainsalloftheAtlantadirectionsobservedand with only two horizontal directions (M=2) and run the
their activation labels at each frame. Concretely, given the detection step only when the number of tracked Atlanta
surfacenormalsofthek-th(current)frame,weindependently directions is less than 2 or periodically with a pre-deï¬ned
track and detect the AF. We then associate both the tracked frame interval (>30 in all of our experiments).
and detected AFsVinto one uniï¬ed AF. We call this uniï¬ed Atlanta frame association. The association stage consists
AF the local AF k, describing the Atlanta structure of the ofthreesteps:localassociation,globalassociationandmain-
L
current (local) frame. Given the local AF, we update the
taining the Atlanta constraint. In the local association step,
global AF and its associated activation labels1. Note that we V V
weassociatethetrackedAF k,andthedetectedAF k into
initializetheglobalAFbytheAFdetectedattheï¬rstframe. V T D
one uniï¬ed local AF k for the current frame. In the global
L V
associationstep,weassociatethelocalAF k withtheglobal
1Abinaryactivationlabelisassociatedwitheachdirectionintheglobal V V L
AF , i.e., updating the global AF and its activation
AF.Eachlabelissetto1ifitscorrespondingAtlantadirectionintheglobal G G
AFisactivated,otherwise0. labels, where the global AF is updated by three operations:
1079
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:24:53 UTC from IEEE Xplore.  Restrictions apply. birth,revival,anddeath(seeexampleinFig.5(a)).Asthelast (a) (b)
V
steLpo,cwael aesnsfoocricaetitohne.ACtloannstiadecroinnsgtrtahienttroancktehdeAloFcaVlkAFas thLke. ğœğ‘˜ ğ‘š3 ğ‘¦à·œ3,ğ‘˜
initial local AF, we associate the detected AF Vk wTith the ğ©ğ‘˜
lAoFc.alFAorFeaancdhï¬dnedtepcotetedntdiiarledctiiroenctivoDndsâˆˆforVuDâˆ kp,dawtiengmDtehaesugrleobiatsl ğœğº ğ¯ğ‘¦â„ğ›¼ğ‘˜ğ‘šğ‘šğ‘¦ğ‘˜cosğ›¼ğ‘¦à·œğ‘š2,ğ‘˜+ğ‘§ğ‘˜sinğ›¼ğ‘š
aâˆ n(gÂ·,uÂ·l)arddeinsotatnescethtoe tahnegtlreacbkeetdweAeFn, it.we.o, v(evctDdo,rsv.TtW),ewthheerne Fig. 4: Illustration of the Kalman ï¬lter componğ‘še2nts for AF-
SLAM: (a) Example of detected planes supporting the current AF
associate two directions vd and vt if their distance is less
D T â—¦ (sphere in the middle) overlaid on both the image space and 3D
than an angle threshold Ï„ (we set Ï„ to 5 ). Otherwise, we space.(b)Descriptionsofthestatevectorandmeasurementmodel
consideritasapotentialdirectionfortheglobalassociation; intheKF.c andc representglobalAtlantaoriginandthecurrent
G k
e.g., the orange and black directions in Fig. 3(d) denote the camera center, respectively.
associateddirectionsandthepotentialdirection,respectively.
If the detection step is not performed, we directly consider this section, we introduce the rotation estimation from the
the tracked case as the local AF. estimated AF and then present the proposed AF-SLAM.
Global association. Using our tracking strategy, the A. Atlanta frame-based visual odometry
V
correspondences between the local AF k and the global
L For AF-aware visual odometry (VO), we follow the ro-
AF directions are known. Based on these relationships, we
tation and translation decoupling method [24] but with AF,
can then estimate the camera rotation (see Sec. IV-A) in
which leads to an efï¬cient structure-aware camera motion
the reference of the global AF. In this reference, similarly
estimation. The AF provides a set of vanishing directions
to the local association, we associate the local AF with the
that can be used for a robust estimation of the rotation of
global AF using the corresponding angular distance. This
the camera independently of the translation.
process is as follows. Death operation: ï¬rst, we de-activate
Rotation estimation by rotation averaging. Using our
non-associateddirectionsintheglobalAFw.r.t.thelocalAF.
associationstrategy(cf.,Sec.III),wehavetheupdatedglobal
Revival operation: we then activate the associated directions
V V
in the global AF if they were not activated; otherwise, no AF G and the local AF Lk with their association (match-
ing) relationship at the current frame. With a minimum of
action is taken. Birth operation: if a potential direction in
two matched Atlanta directions, the camera rotation can be
the local AF is associated with none of the directions in the
estimated in the global AF reference. If more directions are
globalAFandliesonthehorizonoftheglobalAF,wecreate
available, a set of rotation matrices can be computed by
a new Atlanta direction in the global AF and make it the
sampling all possible combinations of directionsâ€™ triplets as:
valid direction in the local AF. To insert the new horizontal
V V âˆ’
direction into an existing AF, we project the new direction R = Ëœi Ëœi 1, (1)
i L G
on the horizon and compute Î± by measuring the angle with where VËœi, VËœi âˆˆ R3Ã—3 represent the i-th sampled local AF
the ï¬rst horizontal direction (e.g., Î± in Fig. 3(e)). L G
4 andthecorrespondingsampledglobalAF,respectively.Note
Atlanta world constraint. After the association step, we
V that we always include the vertical direction by default in
initially reï¬ne each direction in the local AF k via mode V V
L the sampling, and we orthogonalize Ëœi and Ëœi to prevent
seeking during the tracking step, after which we enforce L G
a degenerate solution in Eq. (1).3 We generate a set of
the Atlanta constraint. To this end, ï¬rst we determine an
candidate rotations using Eq. (1) and estimate the camera
accurate representative vertical direction,2 which deï¬nes the (cid:88)
rotation by means of single rotation averaging [31], [32].
horizon. We project the dominant horizontal direction with âˆ’ Â· Â·
We used Lp mean rotation w.r.t. d(, ):
themaximumdensityontothehorizonandthensequentially âˆ—
reï¬ne the other horizontal directions while maintaining each S =arâˆˆgmin d(Ri,S)p, (2)
relative alpha angle w.r.t. the dominant horizontal direction. S SO(3)i=1
We repeat this procedure until the local AF converges. wherep=1inourcasetoincreasethedegreeofrobustness
againstoutliers.Thisfacilitatesanaccurateestimationofthe
IV. ATLANTAFRAME-AWARELINEARSLAM camera rotation given the candidate rotations.
Based on the estimated AF (cf., Sec. III), we present a Translation estimation. Given the estimated rotation, the
linear SLAM for Atlanta world. The proposed AF-SLAM relative translation between two consecutive frames is com-
is built upon the existing linear SLAM for the MW (L- puted by minimizing the de-rotated reprojection error of the
SLAM) [3]. However, unlike L-SLAM which is limited to tracked keypoints [33]. For details, readers can refer to [24].
the MW, the proposed approach can describe more general
B. Linear SLAM for Atlanta world
scenes while preserving a linear SLAM formulation. In
Our linear SLAM compensates for the current camera
2Toensureanaccurateverticaldirectionwellsupportedbythehorizontal translationandtheestimatedAtlantamap(thescenestructure
directions, we generate a set of virtual vertical direction hypotheses from
the cross products of combinations of horizontal direction pairs. We then 3IftheverticaldirectiondoesnotexistinthetrackedAF,wetemporally
computearepresentativeverticaldirectionbytheweightedsumoftheinitial generatethevirtualverticaldirectionbycomputingthecrossproductofany
verticaldirectionandthevirtualdirectionset. twohorizontaldirections.
1080
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:24:53 UTC from IEEE Xplore.  Restrictions apply. (a) (b)
AF-SLAM(ours)
L-SLAM
Groundtruth
death
birth
birth
revival Detection Proposed Ground truth
Fig. 5: Evaluation of synthetic sequence: (a) Example of gen- Fig. 6: Qualitative evaluation of two sequences (lr-kt2n and of-
erated data with possible scenarios, in this case birth, death, and kt0n) in the ICL-NUIM dataset [36]. Sampled images and the
revival. (b) From left to right, sampled results by the BnB-based estimated trajectories by L-SLAM and our AF-SLAM.
AF detection [13], the proposed method, and the ground truth. â‰¥
In contrast to the detection method, our approach maintains a directionsv (m 2)ofAFcannotbedirectlyrepresented
hm
consistent association between consecutive frames. asasimplecoordinate;i.e.,1-DdistanceoftheAtlantaplane
cannotberepresentedusingtheaxisofthesinglecoordinate.
represented by AF) according to the previous state informa- As a key contribution, we represent planar features sup-
tion. For simplicity, our AF-SLAM utilizes a linear KF [34] porting additional horizontal directions by the 1-D angle
with the state vector of the camera translation and planes. Î± and the corresponding 1-D distance, where the y-axis
Speciï¬cally, we detect planes supporting the current local is rotated by Î± around the x-axis, for each horizontal
AF estimate and use the 1-D orthogonal distance from the direction. This causes the horizontal direction representation
camerapositiontoeachplaneasthemeasurementintheKF, to be linear without additional non-linear variables in the
referred to as the planar feature (e.g., Fig. 4(a)).4 Given the state variable and enables a linear formulation for Atlanta
ï£® ï£¹
planarfeature(measurement)andtheVOoutput(cf.,Sec.IV- world. Formally, we deï¬ne the observation model H and
A), we predict and update the state vector. measurement mï£¯odel y: ï£º
ï£¯ âˆ’ ï£º
State vector deï¬nition. The rotation estimation by the ï£° m x ï£»
1âˆ’
VO is sufï¬ciently accurate and robust enough to model the âˆ’ m2 âˆ’y âˆˆR
y=ï£® Hx= m ycosÎ± zsinÎ± ï£¹ q where
others (i.e., translation and planar map) as the state vector. 3 2 2
.
.
Thisallowsane(cid:2)fï¬cientlinearformul(cid:3)ation.Thestatevectorx ï£¯ . ï£º
ï£¯âˆ’ Â·Â·Â·ï£º
and its covarian(cid:20)ce P can be exp(cid:21)ressed as: ï£° 1 0 0 1 0 0 ï£»
âˆ’ Â·Â·Â·
x = p(cid:62), m1, Â·Â·Â· , mn (cid:62) âˆˆR3+n and H= 00 âˆ’cos1Î± âˆ’si0nÎ± 00 10 01 Â·Â·Â· âˆˆRqÃ—(3+n), (4)
2 2
P = Ppp Ppm âˆˆR(3+n)Ã—(3+n), (3) ... ... ... ... ... ... ...
P P
where p=[x,y,z](cid:62)mâˆˆpR3 dmenmotes the 3-DoF camera trans- wtiohner(emÎ±â‰¥m2i)saanndaqnigslethdeenï¬unminbgerthoefmma-ttchhehdorAiztolannttaalpdliarneacr-
lation w.r.t. the global Atlanta map coordinate. The map
âˆˆR featuresbetweenthedetectedplanesinthecurrentcoordinate
mi represents the distance of the Atlanta planar feature and the planes of the global Atlanta map6. H projects the
from the global coordinate origin5, and n is the number of statevectorintotheobservedspace;i.e.,theentitiesofyare
Atlanta planes in the global map, as shown in Fig. 4(b).
the observed 1-D distances from the Atlanta planar features
Process model. Given the translation estimated in Sec. IV- w.r.t. the current state vector. In particular, we can linearly
A, we predict the next state simply with the velocity model observe the 1-D distance by Î± (see Fig. 4(b) and the third
(cid:62) (cid:62)
x =x âˆ’ +[âˆ†pkâˆ’ 0 Ã— ] ,whereâˆ†pkâˆ’ istheestimated rowinH).WecomputetheoptimalKalmangainfromPand
k k 1 k 1 1 n âˆ’ k 1
translationbetweenthek and(k 1)-thframes.Notethatwe H using a method introduced earlier [35] and the difference
donotpredicttheplaneoffsetm ofthestatevectorbecause (residual) between y and the measurements, with these then
i
it is represented in the global Atlanta coordinate. used for the KF update of the state vector.
Measurement model. By taking into account the distance
between the measurement and the current camera pose, we
V. EXPERIMENTALRESULTS
update the state vector in the KF. Speciï¬cally, we deï¬ne
the coordinate system of the AW by regarding the planar
Wevalidatetheproposedtracking-by-detectionschemeon
features supporting the vertical direction v and the ï¬rst
1 a synthetic dataset through a comparison with a pure AF
horizontaldirectionv oftheglobalAFasthex-axisandy-
2 detection-based method. We then evaluate the proposed AF-
axis,respectively(seeFig.4(b)).Thisallowsustoprojectthe
SLAM on RGB-D datasets having structured environments:
state vector into the observed space by a simple arithmetic
the ICL-NUIM [36] and TAMU RGB-D [10] datasets. We
operation: subtraction [3]. However, additional horizontal
implement AF-SLAM in MATLAB; we partially use C++
4Theplaneparameterandthecorresponding1-Dorthogonaldistanceare mex-function.AF-SLAMrunsataround15HzonaPCwith
estimatedasdescribedin[3]. an Intel i7-4790K 4.0GHz CPU and 32GB of RAM.
5Notethatmiisdifferentfromthemeasurement,inthatmiiscomputed
fromtheAtlantaworldoriginw.r.t.thei-thplane,whilethemeasurement
iscomputedfromthecurrentcameralocation. 6Forthedetailedplanematchingandmanagement,referto[3].
1081
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:24:53 UTC from IEEE Xplore.  Restrictions apply. TABLE I: Quantitative evaluation on the ICL-NUIM dataset [36]. ATE RMSEs are measured. Lower is better (unit: meter).
Sequence lr-kt0n lr-kt1n lr-kt2n lr-kt3n of-kt0n of-kt1n of-kt2n of-kt3n
ORB-SLAM2[37] 0.010 0.185 0.028 0.014 0.049 0.079 0.025 0.065
DVO-SLAM[38] 0.108 0.059 0.375 0.433 0.244 0.178 0.099 0.079
CPA-SLAM[19] 0.007 0.006 0.089 0.009 â€“ â€“ â€“ â€“
KDP-SLAM[4] 0.009 0.019 0.029 0.153 â€“ â€“ â€“ â€“
LPVO[24] 0.015 0.039 0.034 0.102 0.061 0.052 0.039 0.030
L-SLAM[3] 0.012 0.027 0.053 0.143 0.020 0.015 0.026 0.011
AF-SLAM(ours) 0.014 0.035 0.027 0.117 0.036 0.022 0.027 0.025
(a) (c) Top-down view (d) (f) AL-FS-LSALAMM(ours)
ORB-SLAM2
(b) (e)
Side view
Fig. 7: Evaluation on two representative sequences (Corridor-A-const and Auditorium-const) in the TAMU RGB-D dataset [10]:
(a,d)Sampledimages.(b,e)EstimatedAtlantaplanarmapwithsampledcameratrajectoriesbyourAF-SLAM.(c,f)Estimatedtrajectories
compared to L-SLAM [3] and ORB-SLAM2 [37]. This shows that our AF-SLAM more stabley and accurately estimates the trajectories.
A. Tracking-by-detection validation TAMU RGB-D dataset. The TAMU RGB-D dataset [10]
Synthetic data generation. To evaluate the tracking-by- consists of large-scale real-world man-made environments,
detection scheme under the AW assumption, we generate such as stairs and corridors inside a building. We test
syntheticdataincludingpossiblescenariosintheassociation, our approach on Corridor-A-const and Auditorium-const
in this case death, birth, and revival, as shown in Fig. 5(a). sequences that contain texture-less walls and stairs in planar
Speciï¬cally,wedeï¬netheglobalAFwithM(=4)horizontal environments.Weonlyundertakeaqualitativeevaluation,as
directions in the world coordinate (ï¬ve Atlanta directions in theTAMURGB-Ddatasetdoesnotprovidethegroundtruth.
ForCorridor-A-const(Fig.7â€“left),bothAF-SLAMandL-
total) and generate the surface normal distributions support-
SLAM show accurate results because the sequence satisï¬es
ing the global AF. To mimic natural motions, we generate
the MW assumption. On the other hand, ORB-SLAM2
sequential 3D rotations (600 frames in total) and smooth the
shows drifts due to the lack of texture on the wall, which
resultscontinuouslyusingarotation-smoothingmethod[39].
yields a biased keypoint distribution. Regarding Auditorium-
Giventherotationsandactivationlabels,werotatetheglobal
const (Fig. 7â€“right), L-SLAM lost the MF track when it
AF to generate the ground truth of the local AF.
encounteredanon-Manhattanpartofthescene.Furthermore,
Evaluation. On the generated dataset, we test our tracking-
it could not recover, even after a single failure due to the
by-detection method and the pure BnB-based AF detection
absence of a detection mechanism. ORB-SLAM2 shows
method [13]. As shown in Fig. 5(b), the detection method
a reasonable trajectory in this sequence but accumulates
independently estimates AF that maximizes the number âˆ¼
signiï¬cantdrift( 3m)duetothetexture-lessstairsandwalls
of inliers, thus it exhibits ï¬‚uctuation and does not show
(seethesideviewofthegreen-coloredtrajectory).Byvirtue
consistent associations between consecutive frames. On the
of the proposed tracking-by-detection strategy, our method
other hand, our method shows a stable tracking result while
continues to track the AF while dealing with new Atlanta
identifying new or missed Atlanta directions. For a detailed
directionsofthescene.Thereby,ourAF-SLAMcanrobustly
comparison, readers can refer to the supplementary video.
estimatethecameraposes.Inaddition,theestimatedAtlanta
B. SLAM validation map in Figs. 7(b,e) shows that our method reconstructs the
ICL-NUIM dataset. The ICL-NUIM dataset [36] is a scene structure properly.
synthetic RGB-D dataset rendered in a low-texture living
VI. CONCLUSION
room and ofï¬ce with ground truth camera trajectories. We
We leverage the AW prior for a stable and robust SLAM
compare our AF-SLAM with ORB-SLAM2 [37], DVO-
eveninthepresenceoftexture-lessscenes.WhiletheAtlanta
SLAM[38],CPA-SLAM[19],KDP-SLAM[4],LPVO[24],
prior is much more general than the Manhattan model, our
andL-SLAM[3].Here,weusebothsurfacenormalsandline
keycontributionisthecreationofanAF-basedlinearSLAM
normals for a fair comparison with L-SLAM. We measure
method that favors compact representation and enables a
the RMSE of the absolute trajectory error (ATE) [40].7
robustandefï¬cienttracking-by-detectionalgorithm.Through
Note that this is a fairly advantageous setup for L-SLAM,
experiments in various environments, we demonstrated that
becausethescenesinthisdatasetareperfectMWstructures,
AF allows us to expand successful working regimes favor-
which are highly favorable for L-SLAM. Nonetheless, as
ablyagainstcompetingmethods.Inaddition,ourAF-SLAM
shown in Table I and Fig. 6, our method shows comparable
jointlyestimatescameramotionsaswellastheAtlantaplanar
performance outcomes to L-SLAM in MW scenes.
map as a byproduct, which may open new and interesting
7FromKimetal.[3],wedirectlyquotetheresultsoftheotherapproaches. applications, such as schematic 3D reconstructions.
1082
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:24:53 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [24] P.Kim,B.Coltin,andH.J.Kim,â€œLow-driftvisualodometryinstruc-
turedenvironmentsbydecouplingrotationalandtransnationalmotion,â€
[1] T.Taketomi,H.Uchiyama,andS.Ikeda,â€œVisualSLAMalgorithms:
inIEEEInternationalConferenceonRoboticsandAutomation(ICRA),
Asurveyfrom2010to2016,â€IPSJTransactionsonComputerVision
2018.
andApplications,vol.9,no.1,p.16,2017.
[25] D.Zou,Y.Wu,L.Pei,H.Ling,andW.Yu,â€œStructvio:visual-inertial
[2] P. Kim, B. Coltin, and H. J. Kim, â€œIndoor RGB-D compass from a
odometrywithstructuralregularityofman-madeenvironments,â€IEEE
single line and plane,â€ in IEEE Conference on Computer Vision and
TransactionsonRobotics(TRO),vol.35,no.4,pp.999â€“1013,2019.
PatternRecognition(CVPR),2018.
[26] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, â€œIndoor seg-
[3] P.Kim,B.Coltin,andH.J.Kim,â€œLinearRGB-DSLAMforplanar
mentation and support inference from RGBD images,â€ in European
environments,â€inEuropeanConferenceonComputerVision(ECCV),
ConferenceonComputerVision(ECCV),2012.
2018.
[27] K.Joo,T.-H.Oh,J.Kim,andI.S.Kweon,â€œRobustandgloballyopti-
[4] M. Hsiao, E. Westman, G. Zhang, and M. Kaess, â€œKeyframe-based
malmanhattanframeestimationinnearrealtime,â€IEEETransactions
denseplanarSLAM,â€inIEEEInternationalConferenceonRobotics
onPatternAnalysisandMachineIntelligence(TPAMI),vol.41,no.3,
andAutomation(ICRA),2017.
pp.682â€“696,2018.
[5] P.-H. Le and J. KosË‡ecka, â€œDense piecewise planar RGB-D SLAM
[28] Y. Zhou, L. Kneip, C. Rodriguez, and H. Li, â€œDivide and conquer:
for indoor environments,â€ in IEEE/RSJ International Conference on
Efï¬cientdensity-basedtrackingof3DsensorsinManhattanworlds,â€
IntelligentRobotsandSystems(IROS),2017.
inAsianConferenceonComputerVision(ACCV),2016.
[6] S.Yang,Y.Song,M.Kaess,andS.Scherer,â€œPop-upSLAM:Semantic
[29] Y.Cheng,â€œMeanshift,modeseeking,andclustering,â€IEEETransac-
monocular plane SLAM for low-texture environments,â€ in IEEE/RSJ
tionsonPatternAnalysisandMachineIntelligence(TPAMI),vol.17,
International Conference on Intelligent Robots and Systems (IROS),
no.8,pp.790â€“799,1995.
2016.
[30] Z.Kalal,K.Mikolajczyk,andJ.Matas,â€œTracking-learning-detection,â€
[7] G. Grisetti, R. Kummerle, C. Stachniss, and W. Burgard, â€œA tutorial
IEEE Transactions on Pattern Analysis and Machine Intelligence
on graph-based SLAM,â€ IEEE Intelligent Transportation Systems
(TPAMI),vol.34,no.7,pp.1409â€“1422,2011.
Magazine,vol.2,no.4,pp.31â€“43,2010.
[31] R.Hartley,K.Aftab,andJ.Trumpf,â€œL1rotationaveragingusingthe
[8] M. Kaess, A. Ranganathan, and F. Dellaert, â€œiSAM: Incremental
Weiszfeld algorithm,â€ in IEEE Conference on Computer Vision and
smoothing and mapping,â€ IEEE Transactions on Robotics (TRO),
PatternRecognition(CVPR),2011.
vol.24,no.6,pp.1365â€“1378,2008.
[32] R. Hartley, J. Trumpf, Y. Dai, and H. Li, â€œRotation averaging,â€
[9] L. Carlone, R. Tron, K. Daniilidis, and F. Dellaert, â€œInitialization
InternationalJournalofComputerVision(IJCV),vol.103,no.3,pp.
techniques for 3D SLAM: a survey on rotation estimation and its
267â€“305,2013.
useinposegraphoptimization,â€inIEEEInternationalConferenceon
[33] J.-Y. Bouguet et al., â€œPyramidal implementation of the afï¬ne Lucas
RoboticsandAutomation(ICRA),2015.
Kanade feature tracker description of the algorithm,â€ Intel Corpora-
[10] Y. Lu and D. Song, â€œRobust RGB-D odometry using point and
tion,vol.5,no.1-10,p.4,2001.
linefeatures,â€inIEEEInternationalConferenceonComputerVision
[34] G.WelchandG.Bishop,â€œAnintroductiontotheKalmanï¬lter,â€1995.
(ICCV),2015.
[35] D.Simon,Optimalstateestimation:Kalman,Hinï¬nity,andnonlinear
[11] J.M.CoughlanandA.L.Yuille,â€œManhattanworld:Compassdirec-
approaches. JohnWiley&Sons,2006.
tionfromasingleimagebybayesianinference,â€inIEEEInternational
[36] A. Handa, T. Whelan, J. McDonald, and A. Davison, â€œA benchmark
ConferenceonComputerVision(ICCV),1999.
forRGB-Dvisualodometry,3DreconstructionandSLAM,â€inIEEE
[12] G. Schindler and F. Dellaert, â€œAtlanta world: An expectation max-
InternationalConferenceonRoboticsandAutomation(ICRA),2014.
imization framework for simultaneous low-level edge grouping and
[37] R.Mur-ArtalandJ.D.TardoÂ´s,â€œORB-SLAM2:anopen-sourceSLAM
camera calibration in complex man-made environments,â€ in IEEE
system for monocular, stereo and RGB-D cameras,â€ IEEE Transac-
Conference on Computer Vision and Pattern Recognition (CVPR),
tionsonRobotics(TRO),vol.33,no.5,pp.1255â€“1262,2017.
2004.
[38] C.Kerl,J.Sturm,andD.Cremers,â€œDensevisualSLAMforRGB-D
[13] K. Joo, T.-H. Oh, I. S. Kweon, and J.-C. Bazin, â€œGlobally optimal
cameras,â€inIEEE/RSJInternationalConferenceonIntelligentRobots
inlier set maximization for Atlanta frame estimation,â€ in IEEE Con-
andSystems(IROS),2013.
ferenceonComputerVisionandPatternRecognition(CVPR),2018.
[39] C.JiaandB.L.Evans,â€œConstrained3Drotationsmoothingviaglobal
[14] K. Joo, T.-H. Oh, I. S. Kweon, and J.-C. Bazin, â€œGlobally optimal
manifold regression for video stabilization,â€ IEEE Transactions on
inliersetmaximizationforatlantaworldunderstanding,â€IEEETrans-
SignalProcessing,vol.62,no.13,pp.3293â€“3304,2014.
actionsonPatternAnalysisandMachineIntelligence(TPAMI),2019.
[40] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, â€œA
[15] A.J.Davison,I.D.Reid,N.D.Molton,andO.Stasse,â€œMonoSLAM:
benchmarkfortheevaluationofRGB-DSLAMsystems,â€inIEEE/RSJ
Real-timesinglecameraslam,â€IEEETransactionsonPatternAnalysis
International Conference on Intelligent Robots and Systems (IROS),
andMachineIntelligence(TPAMI),no.6,pp.1052â€“1067,2007.
2012.
[16] A.P.Gee,D.Chekhlov,A.Calway,andW.Mayol-Cuevas,â€œDiscov-
ering higher level structure in visual SLAM,â€ IEEE Transactions on
Robotics(TRO),vol.24,no.5,pp.980â€“990,2008.
[17] J. MartÂ´Ä±nez-Carranza and A. Calway, â€œUnifying planar and point
mappinginmonocularSLAM,â€inBritishMachineVisionConference
(BMVC),2010.
[18] M. Kaess, â€œSimultaneous localization and mapping with inï¬nite
planes,â€ in IEEE International Conference on Robotics and Automa-
tion(ICRA),2015.
[19] L.Ma,C.Kerl,J.StuÂ¨ckler,andD.Cremers,â€œCPA-SLAM:Consistent
plane-model alignment for direct RGB-D SLAM,â€ in IEEE Interna-
tionalConferenceonRoboticsandAutomation(ICRA),2016.
[20] F. Nardi, B. Della Corte, and G. Grisetti, â€œUniï¬ed representation
andregistrationofheterogeneoussetsofgeometricprimitives,â€IEEE
RoboticsandAutomationLetters(RA-L),vol.4,no.2,pp.625â€“632,
2019.
[21] H.Li,Y.Xing,J.Zhao,J.-C.Bazin,Z.Liu,andY.-H.Liu,â€œLeveraging
structuralregularityofAtlantaworldformonocularSLAM,â€inIEEE
InternationalConferenceonRoboticsandAutomation(ICRA),2019.
[22] J. Straub, N. Bhandari, J. J. Leonard, and J. W. Fisher, â€œReal-time
Manhattanworldrotationestimationin3D,â€inIEEE/RSJInternational
ConferenceonIntelligentRobotsandSystems(IROS),2015.
[23] P. Kim, B. Coltin, and H. J. Kim, â€œVisual odometry with drift-free
rotationestimationusingindoorsceneregularities,â€inBritishMachine
VisionConference(BMVC),2017.
1083
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:24:53 UTC from IEEE Xplore.  Restrictions apply. 