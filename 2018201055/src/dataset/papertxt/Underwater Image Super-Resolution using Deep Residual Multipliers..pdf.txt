2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Keyï¬lter-Aware Real-Time UAV Object Tracking
âˆ—
Yiming Li1, Changhong Fu1, , Ziyuan Huang2, Yinqiang Zhang3, and Jia Pan4
Abstractâ€”Correlation ï¬lter-based tracking has been widely ðŸâˆ’ð† â€¦ Training Response map
ð† ð†
applied in unmanned aerial vehicle (UAV) with high efï¬ciency.
#285
However, it has two imperfections, i.e., boundary effect and
Filter
ï¬ltercorruption.Severalmethodsenlargingthesearchareacan
mitigateboundaryeffect,yetintroducingundesiredbackground
Correlation
distraction.Existingframe-by-framecontextlearningstrategies #286
for repressing background distraction nevertheless lower the Keyfilter ð‘›âˆ’1 Feature of 
tracking speed. Inspired by keyframe-based simultaneous lo- Restriction #286
calizationandmapping,keyï¬lterisproposedinvisualtracking
Contextual learning
fortheï¬rsttime,inordertohandletheaboveissuesefï¬ciently Keyfilter ð‘› Correlation
and effectively. Keyï¬lters generated by periodically selected
#285
keyframes learn the context intermittently and are used to
Filter
restrain the learning of ï¬lters, so that 1) context awareness
canbetransmittedtoalltheï¬ltersviakeyï¬lterrestriction,and
2)ï¬ltercorruptioncanberepressed.Comparedtothestate-of- ð† ð† â€¦ Training
ðŸâˆ’ð†
the-art results, our tracker performs better on twochallenging
benchmarks,withenoughspeedforUAVreal-timeapplications. Fig. 1. Comparison between response maps of our tracker and baseline.
Red frames are served as keyframes generating keyï¬lters. Keyï¬lters carry
out context learning intermittently and inï¬‚uence the current ï¬lter training
I. INTRODUCTION formitigatingï¬ltercorruption.Featureofcurrentframeiscorrelatedwith
theï¬ltertrainedinthelastframe,producingaresponsemap.Redandblack
Combined with extensibility, autonomy, and maneuver- rectanglesdenoterespectivelytheresultsfromKAOTandbaseline.
ability of unmanned aerial vehicle (UAV), visual object
tracking has considerable applications in UAV, e.g., person
tracing [1], autonomous landing [2], aerial photography [3], has introduced more context noise, distracting the detection
and aircraft tracking [4]. Notwithstanding some progress, phase especially in situations of similar objects around.
UAV tracking remains onerous because of the complex Inliterature,thecontext-awareframework[14]isproposed
background, frequent appearance variation caused by UAV toreducethecontextdistractionthroughresponserepression
motion,full/partialocclusion,deformation,aswellasillumi- of the context patches. However, the frame-by-frame con-
nationchanges.Besides,computationallyintractabletrackers text learning is extremely redundant, because the capture
are not deployable onboard UAVs because of the harsh frequency of drone camera is generally smaller than the
calculation resources and limited power capacity. frequencyofcontextvariation,e.g.,theintervaltimebetween
Recently,theframeworkofdiscriminativecorrelationï¬lter twoconsecutivetimeina30framepersecond(FPS)videois
(DCF) [5], aiming to discriminate the foreground from the 0.03 second, but generally the context appearance in aerial
background via a correlation ï¬lter (CF), is widely adopted view remains unchanged for a certain time far more than
in UAV object tracking. The speed is hugely raised because 0.03 second. In addition, the learned single ï¬lter without
of its utilization of the circulant matricesâ€™ property to carry restriction is prone to corruption due to the omnipresent
out the otherwise cumbersome calculation in the frequency appearance variations in the aerial scenarios.
domain rather than spatial one. Yet the circulant artiï¬cial In this work, inspired by keyframe-based simultaneous
samples used to train the ï¬lter hamper the ï¬lterâ€™s discrimi- localization and mapping (SLAM) [15], the keyframe tech-
nativeability.Thisproblemiscalledboundaryeffectbecause nique is used to raise the tracking performance efï¬ciently
the artiï¬cial non-real samples have periodical splicing at the and effectively. The contributions of this work are two-fold:
boundary. Several approaches [6]â€“[13] expand the search â€¢ Anovelapplicationofthekeyï¬lterinUAVvisualobject
area for alleviating boundary effects, but the enlargement tracking is presented. Keyï¬lters generated at a certain
frequency learn the context intermittently and enforce
1Yiming Li and Changhong Fu are with the School of Me- temporal restriction. Through the restriction, the ï¬lter
chanical Engineering, Tongji University, 201804 Shanghai, China.
corruption in the time span is alleviated and context
changhongfu@tongji.edu.cn
noise is efï¬ciently suppressed.
2ZiyuanHuangiswiththeAdvancedRoboticsCentre,NationalUniver-
sityofSingapore,Singapore.ziyuan.huang@u.nus.edu â€¢ Extensive experiments on 193 challenging UAV im-
3Yinqiang Zhang is with the Department of Mechanical age sequences have shown that the keyï¬lter-aware
Engineering, Technical University of Munich, Munich, Germany. object tracker, i.e., KAOT, has competent performance
yinqiang.zhang@tum.de
compared with the state-of-the-art tracking approaches
4Jia Pan is with the Computer Science Department, The University of
HongKong,HongKong,China.panjia1983@gmail.com based on DCF and deep neural network (DNN).
978-1-7281-7395-5/20/$31.00 Â©2020 IEEE 193
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. II. RELATEDWORKS III. REVIEWOFBACKGROUND-AWARE
CORRELATIONFILTER
A. Discriminative correlation ï¬lter
The objective function of background-aware correlation
In recent years, the framework of discriminative correla-
ï¬lters (BACF) [12] is(cid:88)as follows: (cid:88)
tion ï¬lter (DCF) [5] has broadly aroused research interest
due to its remarkable efï¬ciency. Yet classic CF-based track- E 1(cid:107) âˆ’ D (cid:107) Î» D (cid:107) (cid:107)
ers [16]â€“[18] have limited performance due to the lack of (w)= y Bxd(cid:63)wd 2+ wd 2 , (1)
2 0 2 2 2
negative samples, i.e., the circulant artiï¬cial samples created d=1 d=1
âˆˆR âˆˆR âˆˆR
to train the CF hugely reduce its discriminative power. One where y M, xd N and wd M denote the desired
solution to this problem is spatial penalization to punish response, the dth one of D feature channels and correlation
E
the ï¬lter value at the boundary [6]â€“[10]. Another solution ï¬lter respectively. Î» is a regularization parameter and (w)
is cropping both the background and target to use negative refers to an error between the desired response y and the
samples in the real word instead of synthetic samples [11]â€“ actual one. (cid:63) is the spatial correlation operator. The main
âˆˆ R Ã—
[13]. However, the aforementioned approaches are prone to idea of BACF is to utilize a cropping matrix B M N
introduce context distraction because of enlarging search to extract real negative samples. However, more background
area, especially in the scenarios of similar object around. distraction is introduced because of the enlargement.
B. Prior work to context noise and ï¬lter corruption IV. KEYFILTER-AWAREOBJECTTRACKER
Inliterature,M.Muelleretal.[14]proposedtorepressthe Inspired by the keyframe technique used in SLAM, the
response of context patches, i.e., the features extracted from keyï¬lter is ï¬rstly proposed in visual tracking to boost ac-
surrounding context are directly fed into classic DCF frame- curacy and efï¬ciency, as illustrated in Fig. 2. The objective
(cid:88) (cid:88)
workandtheirdesiredresponsesaresuppressedaszero.The function of KAOT tracker is written as follows:
contextdistractionisthuseffectivelyrepressed,consequently E 1(cid:107) âˆ’ D (cid:107) Î» D (cid:107) (cid:107)
the discriminative ability of the ï¬lter is enhanced. Neverthe- (w)= 2 y(cid:88) (cid:88)Bxd0(cid:63)wd 22+ 2 (cid:88)wd 22
less, the frame-by-frame context learning is effective but not d=1 d=1 , (2)
efï¬cient, and its redundancy can be signiï¬cantly reduced. S P (cid:107) D (cid:107) Î³ D (cid:107) âˆ’ (cid:107)
+ p Bxd(cid:63)wd 2+ wd wËœd 2
Another problem of classic DCF trackers is that the learned 2 p 2 2 2
p=1 d=1 d=1
single ï¬lter is commonly subjected to corruption because of
wherethethirdtermisresponserepressionofcontextpatches
the frequent appearance variation. Online passive-aggressive (their desired responses are zero), and S is the score of pth
p
learning is incorporated into the DCF framework [19] to patch to measure the necessity of penalization (introduced
âˆˆ R âˆˆ R
mitigate the corruption. Compared to [19], the presented in IV-B). wd M and wËœd M are the current ï¬lter
keyï¬lter performs better in both precision and speed. and keyï¬lter, respectively. Î³ is the penalty parameter of the
gap between wd and wËœd. To improve the calculation speed,
C. Tracking by deep neural network Eq. (2) is calculated in the frequency domain:
Recently, deep neural network has contributed a lot to E 1(cid:107) âˆ’ (cid:107) Î»(cid:107) (cid:107) Î³(cid:107) âˆ’ (cid:107)
(w,gË†)= XË†gË† YË† 2+ w 2+ w wËœ 2
the development of computer vision. For visual tracking, 2 2âˆš 2 2 2 2 , (3)
âŠ— (cid:62)
some deep trackers [20]â€“[22] ï¬ne-tuning the deep network s.t. gË†= N(I FB )w
D
onlineforhighprecisionyetruntooslow(around1fpsona âŠ— (cid:2) âˆˆ R Ã— (cid:3)
high-end GPU) to use in practice. Other methods like deep where(cid:2) is the Kron(cid:3)ecker product and ID D D is an
reinforcementlearning[23],unsupervisedlearning[24],con- identity matrix.Ë†denotes the discrete Fourier tÂ·raÂ·nÂ·sform with
trienpureesseonpteartiaotonr[[286]],heanvde-taol-seondinlceraeransiendgt[h2e5t]raacnkdindgeeapccfeuaratucrye. oYË†rtâˆˆh=oCgonyË†a,lÃ—0m,Â·aÂ·tÂ·ri,x0Fâˆˆ., aXRË†ndT XË†=Ã—p âˆˆ CXË†N0,Ã—SD1Ë†NX(1p,= 0,,S1p,X.Ë†..P,P),,
Among them, incorporating lightweight deep features into gË† DN 1 and w DM 1 are respectively deï¬ned as
(cid:62) Â·Â·Â· (cid:62) (cid:62) Â·Â·Â· (cid:62) (cid:62)
online learned DCF framework has exhibited competitive XË† = [diag(xË†1) , ,diag(xË†D) ], gË† = [gË†1 , ,gË†D ] ,
(cid:62) Â·Â·Â· (cid:62) (cid:62) (cid:62) Â·Â·Â· (cid:62) (cid:62)
performance both in precision and efï¬ciency. wËœ =[wËœ1 , ,wËœD ] and w=[w1 , ,wD ] .
A. Optimization algorithm
D. Tracking for unmanned aerial vehicle
Equation (3) can be optimized via alternating direction
Mechanical vibration, motion blur, limited computation
method of multipliers (ADMM) [30]. The Augmented La-
capacity and rapid movement have made UAV tracking an
grangian form of Eq. (3) is:
extremely demanding task. In literature, the presented UAV-
(cid:0) (cid:1)
tailored tracking methods generally have lower robustness L 1(cid:107) âˆ’ (cid:107) Î»(cid:107) (cid:107) Î³(cid:107) âˆ’ (cid:107)
(w,gË†,Î¶Ë†)= XË†gË† YË† 2+ w 2+ w wËœ 2
and accuracy [4], [27]â€“[29]. In light of ofï¬‚ine training 2 âˆš2 2 2 2 2
on the large-scale image datasets, deep feature for robust +Î¶Ë†(cid:62) gË†âˆ’âˆš N(IDâŠ—FB(cid:62))w , (4)
representationcanimproveperformancesigniï¬cantly,yetthe + Âµ(cid:107)gË†âˆ’ N(I âŠ—FB(cid:62))w(cid:107)2
speedofexistingdeep-featurebasedtrackersmostlyrunslow 2 D 2
even on a high-end GPU [9]. This work aims to improve whereÎ¶Ë†âˆˆCDNÃ—1 istheLagrangianvectorinthefrequency
âˆ—
the speed and accuracy for the deep feature-based DCF domain and Âµ is a penalty parameter. Two subproblems gË†
âˆ—
framework for real-time UAV applications. and w are solved alternatively.
194
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. variation Corrupted BKaAseOliTne 
Filter 
#281 #282 #283 #284 #285 #286
Ours
â€¦
â€¦ #281 #282 #283 #284 #285 #286
Baseline 
ð‘¤à·¥ ð‘¤à·¥ 200 Without keyfilter 
ð‘›âˆ’1 ð‘› Without intermittent context learning 
Keyfilter restriction Contextual Learning T=1 T=5
E 20 T=2 T=6
CL T=3 T=7
10 T=4 T=8
0 Frame 500
Fig.2. IllustrationofadvantagesofKAOT.Withthekeyï¬lterrestriction,theï¬ltercorruptionismitigated,asshownonthetopright.Withthecontext
learning, the distraction is reduced, as shown in the response maps from frame 281 to 286. Set the keyï¬lter update period T as 1-8 frames (learns the
contextevery2-16frames),andtheobjectistrackedsuccessfullyinalleighttrackers,whileFPS(framepersecond)israisedto15.2from9.8,lowering
theredundancyofcontextlearningsigniï¬cantly.Inaddition,trackerslackingofthekeyï¬lterrestrictionorthecontextlearningbothlosethetarget.
â€¢ âˆ— (cid:26) âˆ— (cid:16) (cid:17)
Subproblem w (ï¬lter in the spatial domain): and wË† is obtained through the following formula:
j+1
âˆ— (cid:0) Î»(cid:107) (cid:107) Î³(cid:107) âˆ’ (cid:107)(cid:1) âˆ— âŠ— (cid:62) âˆ—
w =argmin w 2+ w wËœ 2 wË† = I FB w , (10)
w 2 âˆš 2 2 2(cid:111) j+1 D j+1
+Î¶Ë†(cid:62) gË†âˆ’âˆš N(IDâŠ—FB(cid:62))w subscriptjdenotesthethevalueatlastiterationandsubscript
(cid:18)Âµ(cid:107) âˆ’ (cid:19) âŠ— (cid:62) (cid:107) . (5) j+1 denotes the value at current iteration.
+ gË† N(I FB )w 2
2 D 2
âˆ’ B. Context patches scoring scheme
Î»+Î³ 1 Î³
= Âµ+ (Âµg+Î¶+ wËœ) This work adopts a simple but effective scheme for
N N
â€¢ âˆ— (cid:26) measuring the score of context patches through Euclidean
Subproblem gË† (ï¬lter in the frequency domain): distance. Speciï¬cally, the size of omni-directional patches
âˆ— (cid:0) 1(cid:107) âˆ’ (cid:107) (cid:1) located around the object is the same as that of the object.
gË† =argmin XË†gË† YË† 2
gË† 2 âˆš 2 (cid:111) The score of patch p is calculated as follows:
+Î¶Ë†(cid:62) gË†âˆ’âˆš N(IDâŠ—FB(cid:62))w . (6) min{w,h}
+Âµ(cid:107)gË†âˆ’ N(I(cid:2) âŠ—FB(cid:62))w(cid:107)2 (cid:3) Sp = |OO | s , (11)
(cid:2) 2(cid:0) (cid:1) D (cid:0) 2(cid:1)(cid:3) p
| |
(cid:62) where OO denotes the Euclidean distance between the
yË†(n)onlydependsonxË†(n)= xË†1(n),xË†2(n),...,xË†D(n) p
(cid:62) object and context patch p (p=1,2,...,P) (between center
and gË†(n) = conj gË†1âˆ—(n) ,...,conj gË†D(n) . Hence, points) and s is the base score which is a constant number.
solving equation(cid:26)for gË† can be identically written as N w, h are respectively the width and height of the object
separate functions gË†(n) (n=[1,...,N]): rectangle. Through Eq. (11) , the patch which is closer to
âˆ— 1(cid:107) âˆ’ (cid:62) (cid:107) object, obtains a higher score for stronger penalization.
gË†(n) =ar(cid:88)gmin yË†(n) xË† (n) gË†(n) 2
gË†(n) 2 0 2 C. Keyï¬lter updating strategy
+ 1 P (cid:107)S xË† (n)(cid:62)gË†(cid:111)(n)(cid:107)2+Î¶Ë†(n)(cid:62)(gË†(n)âˆ’wË†(n)), (7) Startingfromtheï¬rstframe,thekeyï¬lterisgeneratedata
2 p p 2
certainfrequencyusingkeyframesandcurrentkeyï¬lterrefers
p=1 (cid:2) (cid:3)
+Âµ(cid:107)gË†(n)âˆ’wË†(n)(cid:107)2 to the latest trained keyï¬lter, as shown in Fig. 2. Current
2 2 ï¬lterisrestrictedbycurrentkeyï¬lterthroughthepunishment
wâˆšhere wË†(n) = wË†1(n),...,wË†D(n) and wË†d = introduced by the gap between current ï¬lter and keyï¬lter. In
DFP(cid:62)wd. The(cid:88)solution to each sub-subproblem is: otherwords,currentkeyï¬lterisupdatedeverycframes(c=
8 in this work). When the (n+1)th keyframe arrives (frame
âˆ— P (cid:62) âˆ’ Ã—
gË†(n) =( S2xË† (n)xË† (n) +ÂµI ) 1 k =c n+1), the ï¬lter of current frame (keyï¬lter (n+1))
p=0 p p p D . (8) is trained under inï¬‚uence from the keyï¬lter n. As for the
âˆ’
(yË†(n)xË†(n) Î¶Ë†(n)+ÂµwË†(n)) non-keyframes after keyï¬lter (n+1), the ï¬lters of them are
0
(cid:0) (cid:1) learnedwiththerestrictionofcurrentkeyï¬lter(keyï¬lter(n+
Lagrangian parameter is updated as follows:
1)).Thedetailedwork-ï¬‚owofKAOTtrackerispresentedin
âˆ— âˆ’ âˆ—
Î¶Ë† =Î¶Ë† +Âµ gË† wË† , (9) Algorithm 1.
j+1 j j+1 j+1
195
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. Precision plots on DTB70 Success plots on DTB70
KAOT [0.757] KAOT [0.503]
0.8 AECROC F[0 [.07.2629]4] 0.8 AECROC F[0 [.05.0427]2]
UDT+ [0.658] UDT+ [0.462]
STRCF [0.649] ECO_HC [0.453]
CSRDCF [0.646] CSRDCF [0.438]
0.6 ECO_HC [0.643] e0.6 STRCF [0.437]
n CF2 [0.616] at UDT [0.422]
Precisio0.4 fBUMDADCSCTCS FTT[ 0 _[[.0H06..05 5[2930]04.6]]04] uccess r0.4 fBCMDAFCS2CCS [FTT0  ._[[40H01..45 3[0]5027.4]]05]
SAMF [0.519] S Staple_CA [0.351]
Staple_CA [0.504] SAMF [0.340]
0.2 CFNet [0.475] 0.2 CFNet [0.322]
KCF [0.468] KCC [0.291]
DCF [0.467] KCF [0.280]
KCC [0.440] DCF [0.280]
0 Staple [0.365] 0 Staple [0.265]
0 10 20 30 40 50 0 0.2 0.4 0.6 0.8 1
Location error threshold Overlap threshold
Precision plots on UAV123@10fps Success plots on UAV123@10fps
0.8 0.8
ECO [0.711] ECO [0.520]
KAOT [0.686] KAOT [0.479]
UDT+ [0.675] UDT+ [0.478]
ARCF [0.666] ARCF [0.473]
0.6 CSRDCF [0.643] 0.6 ECO_HC [0.462]
ECO_HC [0.634] STRCF [0.457]
STRCF [0.627] e CSRDCF [0.450]
n CF2 [0.601] at MCCT_H [0.433]
Precisio0.4 BUSMtADaCpCTCl FeT[_ 0_[C.0H5A.75 [5 7[0]20.5].59867]] uccess r0.4 BCUStAFDa2pCT l[ Fe[0_ 0.[C4.042A.3450 1][]30].420]
KCC [0.531] S fDSST [0.379]
0.2 fCDFSNSeTt  [[00..552156]] 0.2 CKFCNCe [t0 [.03.7347]3]
SAMF [0.466] Staple [0.342]
Staple [0.456] SAMF [0.326]
DCF [0.408] DCF [0.266]
0 KCF[0.406] 0 KCF [0.265]
0 10 20 30 40 50 0 0.2 0.4 0.6 0.8 1
Location error threshold Overlap threshold
Fig.3. Precisionandsuccessplotsbasedonone-pass-evaluation[31]ofKAOTandotherreal-timetrackersonDTB70[32]andUAV123@10fps[33].
  Algorithm 1: KAOT tracker V. EXPERIMENTS
âˆ’
Input: Location of tracked object on frame k 1,
            In this section, the presented KAOT tracker is rig-
Current keyï¬lter wËœ,
         orously evaluated on two difï¬cult UAV datasets, i.e.,
Keyï¬lter updating Stepsize.
         DTB70 [32] and UAV123@10ps [33], with overall 193
Output: Location and scale of object on frame k
         image sequences captured by drone camera. The tracking
1 for i=2 to end do
         results are compared with the state-of-the-art trackers in-
2 Extract features from the region of interest (ROI)
         cluding both real-time (>=12 FPS) and non-real-time (<
3 Convolute gË† âˆ’ with xË†i on different scales to
          k 1 detect 12FPS) ones, i.e., ARCF [13], UDT [24], UDT+ [24],
generate response maps
         MCCT [34], MCCT-H [34], CSR-DCF [10], STRCF [19],
4 Find the peak position of map and output
          DeepSRTCF [19], ECO [8], ECO-HC [8], BACF [12], Sta-
5 Update object model
           Ã— ple [16], Staple-CA [14], CF2 [26], DCF [14], DSST [35],
6 if k mod Stepsize 2==0 then
           KCF [5], KCC [36], SAMF [17], ADNet [23], CFNet [25],
7 Calculate S (p=1,2,...,8) by Eq. (11)
          p MCPF [37], IBCCF [38]. This work evaluates the trackers
8 Learn CF w by Eq. (5), Eq. (8) and Eq. (9)
          k based on protocol in two datasets respectively [32], [33].
9 wËœ =w
          k Noted that the real-time trackers are trackers with enough
else
10
     speed for UAV real-time applications.
11 if k mod Stepsize ==0 then
12 S =0 (p=1,2,...,8)
p
   13 Learn w by Eq. (5), (8) and Eq. (9) A. Implementation details
k
           14 wËœ =wk KAOT adopts both the hand-crafted and deep features,
else
         15 i.e., histogram oriented gradient (HOG) [39], color name
           16 Sp =0 (p=1,2,...,8) (CN) [40] and conv3 layer from VGG-M network [41]. The
                17 Learn wk by Eq. (5), Eq. (8) and Eq. (9) value of Î³ is set as 10, and the base score s is set as
end
            18 0.28. ADMM iteration is set to 2 for raising efï¬ciency. All
end
          19 trackers are implemented in MATLAB R2018a and all the
           20 Start detection of next frame experimentsareconductedonthesamecomputerwithani7-
end
         21 8700K processor (3.7GHz), 48GB RAM and NVIDIA GTX
            2080 GPU. It is noted that the original codes without any
         modiï¬cation are employed in this work for fair comparison.
196
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. Fast camera motion (41) Background clutter (13) Motion blur (27)
KAOT [0.780] KAOT [0.736] KAOT [0.738]
0.8 EACROC F[0 [.07.4724]2] 0.8 SETCROC [F0 .[607.56]11] 0.8 SETCROC [F0 .[701.86]89]
STRCF [0.713] CSRDCF [0.610] ARCF [0.675]
CSRDCF [0.710] CF2 [0.603] ECO_HC [0.640]
0.6 UECDOT_+H [0C. 6[808.6]80] 0.6 AECROC_FH [0C. 5[805.5]67] 0.6 UBADCT+F  [[00..663398]]
Precision0.4 fCUBMDFADCS2CTCS [ FTT[0 0 ._[[6.0H061..263 5[03]80]67.6]]21] Precision0.4 SUBUMAADDCMCTTC +FFT[  0 _[[[.000H4...3553 [34650]548.4]]]84] Precision0.4 fCUCMDFSDCS2RTCS D[ TT[0C0 ._[5.FH055 .79 [4[90]40].4.65]3062]]
Staple_CA [0.535] fDSST [0.356] SAMF [0.403]
SAMF [0.526] Staple_CA [0.336] Staple_CA [0.387]
0.2 KCF [0.470] 0.2 KCC [0.293] 0.2 DCF [0.345]
DCF [0.469] CFNet [0.286] KCF [0.345]
KCC [0.456] DCF [0.280] CFNet [0.299]
CFNet [0.455] KCF [0.280] KCC [0.298]
0 Staple [0.373] 0 Staple [0.260] 0 Staple [0.248]
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
Location error threshold Location error threshold Location error threshold
In-plane rotation (47) Deformation (18) Similar objects around (27)
KAOT [0.702] KAOT [0.728] ECO [0.803]
0.8 AECROC F[0 [.06.3673]6] 0.8 UADRCT F[ 0[.06.0695]4] 0.8 AKRACOFT [ 0[0.7.73905]]
CSRDCF [0.602] ECO [0.592] UDT+ [0.691]
STRCF [0.586] ECO_HC [0.584] STRCF [0.677]
0.6 UECDOT_+H [0C. 5[803.5]68] 0.6 SCTSRRCDFC F[0 [.505.546]1] 0.6 CECF2O [_0H.6C7 5[0].667]
Precision0.4 fBUCMDAFDCS2CTCS [ FTT[0 0 ._[[5.0H055..557 4[74]80]79.5]]51] Precision0.4 SCCUMAFFDCN2MTC e[+FT0t   ._[[[5000H0...5449 [576]0184.5]]]50] Precision0.4 fUCBMDSADCSRCTCSD FTT[C 0 _[[.0FH05. .76 [5[70260].42.66]]1046]]
SAMF [0.450] BACF [0.448] SAMF [0.552]
Staple_CA [0.439] fDSST [0.390] Staple_CA [0.538]
0.2 DCF [0.416] 0.2 Staple_CA [0.379] 0.2 KCF [0.497]
KCF [0.416] KCC [0.335] DCF [0.496]
CFNet [0.411] KCF [0.302] CFNet [0.490]
KCC [0.397] DCF [0.301] KCC [0.470]
0 Staple [0.309] 0 Staple [0.281] 0 Staple [0.416]
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
Location error threshold Location error threshold Location error threshold
Fig.4. Attributebasedevaluationonprecision.KAOTranksï¬rstplaceonï¬veoutofsixchallengingattributes.
TABLEI
AVERAGEPRECISION(THRESHOLDAT20PIXELS)ANDSPEED((FPS,*MEANSGPUSPEED,OTHERWISECPUSPEED))OFTOPTENREAL-TIME
TRACKERS.RED,GREEN,ANDBLUEFONTSRESPECTIVELYINDICATESTHEBEST,SECOND,ANDTHIRDPLACEINTENTRACKERS.
KAOT ECO[8] ARCF[13] UDT+[24] STRCF[19] CSRDCF[10] ECO_HC[8] CF2[26] MCCT_H[34] UDT[24]
Avg.precision 72.2 71.7 68.0 66.5 63.8 63.5 64.3 62.5 60.3 58.9
Speed(FPS) 14.7* 11.6* 15.3 43.4* 26.3 11.8 62.19 14.4* 59.0 57.5*
B. Comparison with real-time trackers # 000001 # 000155 # 000212
1) Overall performance: Figure 3 demonstrates the over-
all performance of KAOT with other state-of-the-art real-
time trackers on DTB70 and UAV123@10fps. On DTB70
# 000001 # 000016 # 000135
dataset, KAOT (0.757) has an advantage of 4.4% and 9.1%
over the second and third best tracker ECO (0.722), ARCF
(0.694) respectively in precision, along with a gain of 0.2%
and6.6%overthesecond(ECO,0.502)andthirdbesttracker
# 000001 # 000065 # 000115
(ARCF, 0.472) respectively in AUC. On UAV123@10fps
dataset, KAOT (0.686, 0.479) ranks second place followed
by the third place UDT+ (0.675, 0.478). ECO is the only
trackerperformingbetterthanKAOT.Nevertheless,itutilizes
# 000001 # 000065 # 000116
continuous operator to fuse the feature maps elaborately,
while KAOT just uses the simple BACF as baseline. Notice
that ECO can further enhance its performance with our
framework.Averageprecisiononthetwodatasetsandspeed # 000001 # 000135 # 000204
(evaluatedonDTB70)arereportedinTableI.KAOTis27%
faster than ECO when achieving higher precision.
Discussions: DTB70 [32] dataset is recorded on a drone
with more frequent and drastic displacements compared KAOT ECO ECO-HC ARCF UDT+
to UAV123@10fps [33], thus increasing the tracking dif- Fig. 5. Qualitative evaluation. From the top to bottom is re-
ï¬culties. Our method exhibits relatively big advantages on spectively the sequence ChasingDrones, RcCar6, SnowBoarding2,
DTB70, proving the robustness of our method in the scenar- Gull1 and wakeboard2. Code and UAV tracking video are: https:
//github.com/vision4robotics/KAOT-tracker and https:
ios of strong motion.
//youtu.be/jMfmHVRqv3Y.
2) Attribute-based performance: Precision plots of six
challenging attributes are demonstrated in Figure 4. In the
cases of background clutter, KAOT improves the ECO by suppress the background distraction effectively. In situations
9.0% in light of the intermittent context learning which can ofin-planerotationanddeformation,KAOThasasuperiority
197
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. of 10.2% and 23.0% respectively compared to ECO. This is 400
ECO
attributed to the keyï¬lter restriction, which can prevent the CLE  200 EAUCRDOCT-+FHC
ï¬lter from aberrant variation. In addition, KAOT exhibits KAOT
0
excellent performance in the scenario of fast camera motion 0 20 40 60 80 100 120 140 160 180 200
Frame
and motion blur, which is desirable in aerial tracking.
3) Qualitative evaluation: Qualitative tracking results on 1000
ECO
ï¬ve difï¬cult UAV image sequences are shown in Figure 5. LE 500 EACROC-FHC
C  UDT+
Besides,therespectivecenterlocationerror(CLE)variations KAOT
0
of ï¬ve sequences are visualized in Figure 6. Speciï¬cally, in 0 20 40 60 80 100 120 140 160 180 200
Frame
ChasingDrones sequence where tracking is bothered by
strong UAV motion, KAOT has effectively repressed the
20 ECO
distraction of the context, so it can perform well despite CLE  10 EAUCRDOCT-+FHC
the large movement in a certain complex context. Only the   KAOT
0
pre-trained UDT+ tracks successfully in addition to KAOT. 0 10 20 30 40 50 60 70 80 90 100 110
Frame
MotionbluroccursinsequencesRcCar6andGull1(severe
example is shown at frame 16 in RcCar6). In this situation, 200 ECO
ECO-HC
KAOThaskepttrackingowingtothemitigatedï¬ltercorrup- CLE  100 AURDCT+F
tion. As for the last two sequences, keyï¬lter restriction and KAOT
0
intermittentcontextlearninghavecollaborativelycontributed 0 20 40 60 80 100 120
Frame
to successful tracking.
1000
C. Comparison with non-real-time trackers ECO
LE 500 EACROC-FHC
KAOT is also compared with ï¬ve non-real-time trackers C  KUDATO+T
usingdeepneuralnetwork,asshowninTableII.Tosumup, 00 50 100 150 200
Frame
KAOT has the best performance in terms of both precision
and speed on two benchmarks. In addition, compared to Fig.6. IllustrationofCLEvariations.Fromtoptobottomistheresult
DeepSTRCF(usingthesamefeaturesasKAOT),ourtracker from sequence ChasingDrones, RcCar6, SnowBoarding2, Gull1
andwakeboard2,respectively.
has more robust performance in precision on both two
datasets and is around 2.4 times faster than it. Therefore,
the efï¬ciency and accuracy of KAOT tracker can be proven.
deformation,etc.,itisstilllimitedwhentheobjectdisappear
for a long time. Also, KAOT can not handle the rotation
D. Limitations and future works situations. Thus the re-detection and rotation-aware modules
can be added to raise the performance.
Keyframe selection: This work only adopts a simple pe-
Speed: The speed of KAOT is around 15 fps with a GPU
riodic keyframe selection mechanism, which is possible to
and can be used in real-time applications. However, KAOT
introduce distraction when the tracking on the keyframes
tracker is implemented on MATLAB platform and the code
is not reliable. More elaborated strategy can be employed
is not optimized, so the speed can be further improved.
to adaptively choose the keyframe and further enhance the
robustness.
VI. CONCLUSIONS
Re-detection and rotation: Though KAOT performs favor-
ably in the situations of drastic appearance change like blur, This work proposes keyï¬lter-aware object tracker to
repress the ï¬lter corruption and lower the redundancy of
TABLEII context learning. Extensive experiments on two authoritative
PRECISION,SUCCESSRATE(THEAREAUNDERTHECURVE),ANDFPS datasets have validated our tracker performs favorably in
OFKAOTASWELLASFIVENON-REAL-TIMETRACKERS.RED,GREEN, precision,withenoughspeedforreal-timeapplications.This
ANDBLUEFONTSRESPECTIVELYINDICATESTHEBEST,SECOND,AND keyï¬lter-aware framework and intermittent context learning
THIRDPERFORMANCE. strategy can also be used in other trackers like C-COT [7]
and STRCF [19] to further boost their performance. We
DTB70 UAV123@10fps
strongly believe that our method can be used in practice
FPS
Trackers Prec. AUC Prec. AUC and promote the development of UAV tracking applications.
MCPF[37] 66.4 43.3 66.5 44.5 0.57*
MCCT[34] 72.5 48.4 68.4 49.2 8.49*
ACKNOWLEDGMENT
DeepSTRCF[19] 73.4 50.6 68.2 49.9 6.18*
IBCCF[38] 66.9 46.0 65.1 48.1 2.28* This work is supported by the National Natural Sci-
ADNet[23] 63.7 42.2 62.5 43.9 6.87* ence Foundation of China (No. 61806148) and the Fun-
KAOT 75.7 50.3 68.6 47.9 14.69* damental Research Funds for the Central Universities (No.
22120180009).
198
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [20] N. Wang, S. Li, A. Gupta, and D.-Y. Yeung, â€œTransferring
rich feature hierarchies for robust visual tracking,â€ arXiv preprint
arXiv:1501.04587,2015.
[1] H. Cheng, L. Lin, Z. Zheng, Y. Guan, and Z. Liu, â€œAn autonomous
[21] H. Nam and B. Han, â€œLearning multi-domain convolutional neural
vision-based target tracking system for rotorcraft unmanned aerial
networksforvisualtracking,â€inProceedingsoftheIEEEconference
vehicles,â€ in 2017 IEEE/RSJ International Conference on Intelligent
oncomputervisionandpatternrecognition,2016,pp.4293â€“4302.
RobotsandSystems(IROS). IEEE,2017,pp.1732â€“1738.
[22] L.Wang,W.Ouyang,X.Wang,andH.Lu,â€œVisualtrackingwithfully
[2] C. Fu, A. Carrio, M. A. Olivares-Mendez, and P. Campoy, â€œOnline
convolutional networks,â€ in Proceedings of the IEEE international
learning-basedrobustvisualtrackingforautonomouslandingofUn-
conferenceoncomputervision,2015,pp.3119â€“3127.
mannedAerialVehicles,â€inProceedingsofInternationalConference
[23] S. Yun, J. Choi, Y. Yoo, K. Yun, and J. Y. Choi, â€œAction-Decision
onUnmannedAircraftSystems(ICUAS),2014,pp.649â€“655.
Networks for Visual Tracking with Deep Reinforcement Learning,â€
[3] M. Gschwindt, E. Camci, R. Bonatti, W. Wang, E. Kayacan, and
in IEEE Conference on Computer Vision and Pattern Recognition
S. Scherer, â€œCan a Robot Become a Movie Director? Learning
(CVPR),July2017,pp.1349â€“1358.
Artistic Principles for Aerial Cinematography,â€ in 2019 IEEE/RSJ
[24] N.Wang,Y.Song,C.Ma,W.Zhou,W.Liu,andH.Li,â€œUnsupervised
International Conference on Intelligent Robots and Systems (IROS).
Deep Tracking,â€ in The IEEE Conference on Computer Vision and
IEEE,2019.
PatternRecognition(CVPR),2019.
[4] C.Fu,A.Carrio,M.A.Olivares-MÃ©ndez,R.Suarez-Fernandez,and
[25] J. Valmadre, L. Bertinetto, J. Henriques, A. Vedaldi, and P. H.
P.C.Cervera,â€œRobustReal-timeVision-basedAircraftTrackingFrom
Torr, â€œEnd-to-end representation learning for correlation ï¬lter based
Unmanned Aerial Vehicles,â€ in Proceedings of IEEE International
tracking,â€inProceedingsoftheIEEEConferenceonComputerVision
Conference on Robotics and Automation (ICRA), 2014, pp. 5441â€“
andPatternRecognition,2017,pp.2805â€“2813.
5446.
[26] C.Ma,J.Huang,X.Yang,andM.Yang,â€œHierarchicalConvolutional
[5] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista, â€œHigh-Speed FeaturesforVisualTracking,â€in2015IEEEInternationalConference
Tracking with Kernelized Correlation Filters,â€ IEEE Transactions on onComputerVision(ICCV),2015,pp.3074â€“3082.
PatternAnalysisandMachineIntelligence,vol.37,pp.583â€“596,2015. [27] Y. Yin, X. Wang, D. Xu, F. Liu, Y. Wang, and W. Wu, â€œRo-
[6] M. Danelljan, G. HÃ¤ger, F. S. Khan, and M. Felsberg, â€œLearning bustVisualDetection-Learning-TrackingFrameworkforAutonomous
Spatially Regularized Correlation Filters for Visual Tracking,â€ in AerialRefuelingofUAVs,â€IEEETransactionsonInstrumentationand
Proceedings of IEEE International Conference on Computer Vision Measurement,vol.65,pp.510â€“521,2016.
(ICCV),2015,pp.4310â€“4318. [28] C.Yuan,Z.Liu,andY.Zhang,â€œUAV-basedforestï¬redetectionand
[7] M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg, â€œBeyond tracking using image processing techniques,â€ in 2015 International
Correlation Filters: Learning Continuous Convolution Operators for ConferenceonUnmannedAircraftSystems(ICUAS),2015,pp.639â€“
VisualTracking,â€inEuropeanConferenceonComputerVision,2016, 643.
pp.472â€“488. [29] C. Martinez, I. F. Mondragon, P. C. Cervera, J. L. Sanchez-Lopez,
[8] M.Danelljan,G.Bhat,F.S.Khan,andM.Felsberg,â€œECO:Efï¬cient and M. A. Olivares-Mendez, â€œA Hierarchical Tracking Strategy for
Convolution Operators for Tracking,â€ in 2017 IEEE Conference on Vision-Based Applications On-Board UAVs,â€ Journal of Intelligent
Computer Vision and Pattern Recognition (CVPR), 2017, pp. 6931â€“ andRoboticSystems,vol.72,pp.517â€“539,2013.
6939. [30] S.Boyd,N.Parikh,E.Chu,B.Peleato,andJ.Eckstein,â€œDistributed
[9] C.Fu,Z.Huang,Y.Li,R.Duan,andP.Lu,â€œBoundaryEffect-Aware Optimization and Statistical Learning via the Alternating Direction
VisualTrackingforUAVwithOnlineEnhancedBackgroundLearning MethodofMultipliers,â€inFoundationsandTrendsinMachineLearn-
andMulti-FrameConsensusVeriï¬cation,â€inProceedingsofIEEE/RSJ ing,vol.3,2010,pp.1â€“122.
International Conference on Intelligent Robots and Systems (IROS), [31] Y.Wu,J.Lim,andM.-H.Yang,â€œObjectTrackingBenchmark,â€IEEE
2019. Transactions on Pattern Analysis and Machine Intelligence, vol. 37,
[10] A. LukeÅ¾ic, T. VojÃ­r, L. C. Zajc, J. Matas, and M. Kristan, â€œDis- pp.1834â€“1848,2015.
criminative correlation ï¬lter with channel and spatial reliability,â€ in [32] S. Li and D.-Y. Yeung, â€œVisual object tracking for unmanned aerial
2017IEEEConferenceonComputerVisionandPatternRecognition vehicles:Abenchmarkandnewmotionmodels,â€inAAAI,2017.
(CVPR),July2017,pp.4847â€“4856. [33] M.Mueller,N.Smith,andB.Ghanem,â€œABenchmarkandSimulator
[11] H. Kiani Galoogahi, T. Sim, and S. Lucey, â€œCorrelation Filters for UAV Tracking,â€ in Proceedings of European Conference on
With Limited Boundaries,â€ in Proceedings of IEEE Conference on ComputerVision(ECCV),2016,pp.445â€“461.
ComputerVisionandPatternRecognition(CVPR),2015,pp.1â€“9. [34] N.Wang,W.Zhou,Q.Tian,R.Hong,M.Wang,andH.Li,â€œMulti-cue
[12] H. K. Galoogahi, A. Fagg, and S. Lucey, â€œLearning Background- Correlation Filters for Robust Visual Tracking,â€ in 2018 IEEE/CVF
AwareCorrelationFiltersforVisualTracking,â€inProceedingsofIEEE Conference on Computer Vision and Pattern Recognition, 2018, pp.
InternationalConferenceonComputerVision(ICCV),2017,pp.1144â€“ 4844â€“4853.
1152. [35] M.Danelljan,G.HÃ¤ger,F.S.Khan,andM.Felsberg,â€œDiscriminative
[13] Z. Huang, C. Fu, Y. Li, F. Lin, and P. Lu, â€œLearning Aberrance Scale Space Tracking,â€ IEEE Transactions on Pattern Analysis and
RepressedCorrelationFiltersforReal-TimeUAVTracking,â€in2019 MachineIntelligence,vol.39,no.8,pp.1561â€“1575,2017.
IEEEInternationalConferenceonComputerVision(ICCV),2019. [36] C.Wang,L.Zhang,L.Xie,andJ.Yuan,â€œKernelcross-correlator,â€in
[14] M. Mueller, N. Smith, and B. Ghanem, â€œContext-Aware Correlation Thirty-SecondAAAIConferenceonArtiï¬cialIntelligence,2018.
Filter Tracking,â€ in Proceedings of IEEE Conference on Computer [37] T.Zhang,C.Xu,andM.-H.Yang,â€œMulti-taskcorrelationparticleï¬lter
VisionandPatternRecognition(CVPR),2017,pp.1387â€“1395. forrobustobjecttracking,â€inProceedingsoftheIEEEConferenceon
ComputerVisionandPatternRecognition,2017,pp.4335â€“4343.
[15] R. Mur-Artal, J. M. M. Montiel, and J. D. TardÃ³s, â€œORB-SLAM: A
[38] F. Li, Y. Yao, P. Li, D. Zhang, W. Zuo, and M. Yang, â€œIntegrating
VersatileandAccurateMonocularSLAMSystem,â€IEEETransactions
Boundary and Center Correlation Filters for Visual Tracking with
onRobotics,vol.31,pp.1147â€“1163,2015.
Aspect Ratio Variation,â€ in 2017 IEEE International Conference on
[16] L. Bertinetto, J. Valmadre, S. Golodetz, O. Miksik, and P. H. Torr,
ComputerVisionWorkshops(ICCVW),Oct2017,pp.2001â€“2009.
â€œStaple: Complementary Learners for Real-Time Tracking,â€ in Pro-
[39] N.DalalandB.Triggs,â€œHistogramsofOrientedGradientsforHuman
ceedings of the IEEE conference on Computer Vision and Pattern
Detection,â€inProceedingsofIEEEComputerSocietyConferenceon
Recognition(CVPR),2016,pp.1401â€“1409.
Computer Vision and Pattern Recognition (CVPRâ€™05), vol. 1, 2005,
[17] Y.LiandJ.Zhu,â€œAScaleAdaptiveKernelCorrelationFilterTracker
pp.886â€“893.
withFeatureIntegration,â€inProceedingsofEuropeanConferenceon
[40] M.Danelljan,F.S.Khan,M.Felsberg,andJ.vandeWeijer,â€œAdaptive
ComputerVisionWorkshops,2015,pp.254â€“265.
ColorAttributesforReal-TimeVisualTracking,â€2014IEEEConfer-
[18] C.Ma,X.Yang,C.Zhang,andM.-H.Yang,â€œLong-TermCorrelation
ence on Computer Vision and Pattern Recognition, pp. 1090â€“1097,
Tracking,â€ in Proceedings of IEEE Conference on Computer Vision
2014.
andPatternRecognition(CVPR),2015,pp.5388â€“5396.
[41] K.SimonyanandA.Zisserman,â€œVeryDeepConvolutionalNetworks
[19] F. Li, C. Tian, W. Zuo, L. Zhang, and M. Yang, â€œLearning Spatial- for Large-Scale Image Recognition,â€ in Proceedings of International
TemporalRegularizedCorrelationFiltersforVisualTracking,â€in2018 ConferenceonRepresentationLearning,2015,pp.1â€“14.
IEEE/CVFConferenceonComputerVisionandPatternRecognition,
2018,pp.4904â€“4913.
199
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. 