# 基于内容的新闻推荐—DKN算法实践

[TOC]



## 推荐系统及算法简介

### 推荐系统概述

在这个时代，无论是信息消费者还是信息生产者都遇到了很大的挑战:作为信息消费者，如何从大量信息中找到自己感兴趣的信息是一件非常困难的事情;作为信息生产者， 如何让自己生产的信息脱颖而出，受到广大用户的关注，也是一件非常困难的事情。推荐系统就是解决这一矛盾的重要工具。推荐系统的任务就是联系用户和信息，一方面帮助用户发现对自己有价值的信息，另一方面让信息能够展现在对它感兴趣的用户面前，从而实现信息消费者和信息 生产者的双赢。

和搜索引擎不同的是，推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用 户的兴趣建模，从而主动给用户推荐能够满足他们兴趣和需求的信息。



### 推荐算法概述

#### 协同过滤推荐

协调过滤是推荐算法中目前最主流的种类，花样繁多，在工业界已经有了很多广泛的应用。它的优点是**不需要太多特定领域的知识，可以通过基于统计的机器学习算法来得到较好的推荐效果**。最大的优点是工程上容易实现，可以方便应用到产品中。目前绝大多数实际应用的推荐算法都是协同过滤推荐算法。



#### 混合推荐

这个类似机器学习中的集成学习，博才众长，通过多个推荐算法的结合，得到一个更好的推荐算法，起到三个臭皮匠顶一个诸葛亮的作用。比如通过建立多个推荐算法的模型，最后用投票法决定最终的推荐结果。



#### 基于规则的推荐

这类算法常见的比如**基于最多用户点击，最多用户浏览**等，属于大众型的推荐方法，在目前的大数据时代并不主流。



#### 基于人口信息的推荐

这一类是最简单的推荐算法了，它只是简单的根据系统用户的基本信息发现用户的相关程度，然后进行推荐，目前在大型系统中已经较少使用。



#### 基于内容的推荐

这一类一般**依赖于自然语言处理NLP的一些知识，通过挖掘文本的TF-IDF特征向量，来得到用户的偏好**，进而做推荐。这类推荐算法可以找到用户独特的小众喜好，而且还有**较好的解释性**。





## DKN算法简介

DKN算法是一种基于内容的新闻推荐算法。本次实验是目标在于完成DKN算法的搭建。



### 问题定义

新闻标题和正文中通常存在大量的实体，实体间的语义关系可以有效地扩展用户兴趣。然而这种语义关系难以被传统方法（话题模型、词向量）发掘。

对于一个给定的用户user~i~，他的点击历史t = {W~1~，W~2~，……，W~n~}，记为是该用户过去一段时间内曾点击过的新闻的标题，N代表用户点击过的新闻的总数。每个标题都是一个词序列，标题中的单词有的对应知识图谱中的一个实体。举例来说，标题《Trump praises Las Vegas medical team》其中Trump与知识图谱中的实体“Donald Trump”对应，Las和Vegas与实体Las Vegas对应。本文要解决的问题就是给定用户的点击历史，以及标题单词和知识图谱中实体的关联，要预测的是：一个用户i是否会点击一个特定的新闻t~j~。



### DKN模型

<img src="https://recodatasets.blob.core.windows.net/images/dkn_architecture.png" alt="img" style="zoom:80%;" />



上图为DKN模型算法流程图，我们将在下面详细介绍此模型。

DKN的网络输入有两个：候选新闻集合，用户点击过的新闻标题序列。输入数据通过KCNN来提取特征，之上是一个attention层，计算候选新闻向量与用户点击历史向量之间的attention权重，在顶层拼接两部分向量之后，用DNN计算用户点击此新闻的概率。



#### KCNN

##### 简介

Kim CNN，用句子所包含词的词向量组成的二维矩阵，经过一层卷积操作之后再做一次max-over-time的pooling操作得到句子向量。
三种向量**词向量、实体向量、上下文向量**（实体第一层的关联实体的向量），简单的concat，然后通过两种不同size的卷积池化，再将两种size的结果concat。

（１）输入层
如图所示，输入层是句子中的词语对应的wordvector依次排列的矩阵，假设句子有 n 个词，vector的维数为  k  ，那么这个矩阵就是  n × k 的(在CNN中可以看作一副高度为n、宽度为k的图像)。

（２）第一层卷积
输入层通过卷积操作得到若干个Feature Map，卷积窗口的大小为 h ×k ，其中 h  表示纵向词语的个数，而  k  表示word vector的维数。通过这样一个大型的卷积窗口，将得到若干个列数为1的Feature Map。

（３）池化层
接下来的池化层，文中用了一种称为Max-over-time Pooling的方法。这种方法就是简单地从之前一维的Feature Map中提出最大的值，文中解释最大值代表着最重要的信号。可以看出，这种Pooling方式可以解决可变长度的句子输入问题（因为不管Feature Map中有多少个值，只需要提取其中的最大值）。最终池化层的输出为各个Feature Map的最大值们，即一个一维的向量。

##### 具体操作：输入新闻向量（实体+单词）

1、变换：实体向量与上下文向量经过同一个变换，映射到同一线性空间。线性变换/非线性变换？

参数选取w？ b? 是否需要优化？

2、（w1,w2,...,wn,e1,e2,...,en)

3、一个卷积块：多次（这里是3次)不同大小的卷积核卷积，经过relu转换，加入池化层，最后对所有的池化结果连接(concat)并flat。(输出)



#### Attention

##### 简介

简单来说，就是一种权重参数的分配机制，目标是协助模型捕捉重要信息。具体一点就是，给定一组<key,value>，以及一个目标（查询）向量query，attention机制就是通过计算query与每一组key的相似性，得到每个key的权重系数，再通过对value加权求和，得到最终attention数值。
获取到用户点击过的每篇新闻的向量表示以后，计算候选文档对于用户每篇点击文档的attention，再做加权求和，计算attention。

##### 具体操作

1、获取用户点击历史新闻和候选新闻的kcnn特征

2、在判断用户对当前新闻的兴趣时，使用注意力网络（[attention network](https://blog.csdn.net/weixin_36474809/article/details/89401552)）给用户历史记录分配不同的权重。通过softmax函数得到归一化的权重。

在这里，我们用到了一个batch_bomalnization。

batch_normalization：①不仅仅极大提升了训练速度，收敛过程大大加快；②还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等

思想：因为深层神经网络在做非线性变换前的**激活输入值**（就是那个x=WU+B，U是输入）**随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近**（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这**导致反向传播时低层神经网络的梯度消失**，这是训练深层神经网络收敛越来越慢的**本质原因**，**而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布**，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是**这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。**[详细介绍](https://www.cnblogs.com/guoyaohua/p/8724433.html)

3、输出：用户的嵌入 + 候选新闻嵌入



#### DNN

##### 简介

DNN（多层感知机模型）：通过多层全连接层训练，输入attention层输出向量，输出得分。

##### 具体操作

1、随机初始化方法：

Xavier随机初始化 [1]。假设某全连接层的输⼊个数为a，输出个数为b， Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布  
$$
U(-\sqrt{6/(a+b)}, \sqrt{6/(a+b)})
$$
它的设计主要考虑到，模型参数初始化后，每层输出的⽅差不该受该层输⼊个数影响，且每层梯度的⽅差也不该受该层输出个数影响。  

[详细介绍](https://zhuanlan.zhihu.com/p/138064188)



2、丢弃率：防止模型过拟合，效果上呈现为随机丢弃一些计算单元

训练时设定一定的数值

测试时一般设为0，便于得到较好的结果。

[详细介绍](https://blog.csdn.net/program_developer/article/details/80737724)



3、设定优化方法：

adam、adgrad、Gradient Descent、Newton、Momentum……

[详细介绍](https://www.jianshu.com/p/795af312a422)



4、损失函数：数据损失 + 正则化

数据损失：表达预测值与真实值之间的差异: log loss, square loss, cross entropy loss

正则化：给模型加一些规则限制，约束要优化参数，目的是防止过拟合。其中最常见的规则限制就是添加先验约束，其中L1相当于添加Laplace先验，L2相当于添加Gaussian先验。

此次添加的正则化中，有对输入的单词/实体向量的l1 + l2正则化，还有对整个网络上的参数的l1 + l2正则化。



5、分类器常用评测指标：

**auc**（Area Under Curve）被定义为ROC曲线下的面积。我们往往使用AUC值作为模型的评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好

**NDCG**，Normalized Discounted Cumulative Gain , 归一化折损累计增益，是一种考虑了结果相关性和顺序因素的评价指标，可用于多分类问题评价。它的核心思想：
1.高关联度的结果比一般关联度的结果更影响最终的指标得分；
2.有高关联度的结果出现在更靠前的位置的时候，指标会越高。

[详细介绍](https://www.cnblogs.com/chason95/articles/10670761.html)



### 训练过程

1、获取数据（batch训练方式）：一条数据组成是由对于某用户一次行为对某条新闻（候选新闻）的点击状态（1/0）+ 该用户点击过的新闻（用户历史行为）单词向量序号 + 历史点击新闻实体向量序号 + 候选新闻单词向量序号 + 候选新闻实体向量序号。作为feed_dict输入

2、利用已搭建的网络，依次训练。每一个训练周期过后利用评测数据评价到目前为止的准确率。



## 我们目前的工作

项目学习来源于，项目环境搭建也依赖于其中的指示，但是，仅仅针对我们的工作而言，我们并不需要项目中的全部的环境依赖，在项目完成之时，我们会针对我们的需求详细写出一份环境依赖搭建的指示。

我们选择其中examples\02_model_content_based_filtering\dkn_deep_dive.ipynb文件学习并重现其工作内容，不可避免的，我们也会按照我们的需求暂时进行改动。

截止前六周（10.9——11.20），我们通过有关书籍学习深度学习理论知识，利用tensorflow进行网络搭建，并完成了数据准备工作，在项目文件夹src的dataset.py，该文件为一个可执行文件，会将项目数据进行清洗集成并输出至文件中。在此基础上，完成对DKN算法中各部分的实现，学习了其中关于kcnn、神经网络以及各类优化算法等知识，掌握了深度学习的整体框架。

我们将会在以后的提交中展现更多内容，敬请期待……
