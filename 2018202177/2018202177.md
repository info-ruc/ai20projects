# Task 9 HumanEye 终期报告

GroupID: 22  StuID: 2018202177  Name: 官佳薇



**项目概述：对给定图像，生成文字语句对图像内容产生描述。**

在经典图像标注算法Show, Attend and Tell的原理支撑下，完成数据预处理、CNN图像特征抽取（Encoder）、Attention和LSTM框架实现。

项目使用 `PyTorch 1.0`和  `python3.7`



## Objective

在本项目中，我们将首先使用CNN和RNN建立图像标注模型。

以***Show, Attend and Tell*** 算法为基础，使用CNN提取图像特征。

利用RNN生成文字序列，并利用**注意力机制**实现对图像不同区域加权训练。



## Data Preprocessing



#### 数据集描述

数据集采用经典的图像标注数据Fliker8k, Fliker30k, MS COCO。考虑计算资源，采用Fliker8k进行训练和测试。

数据集包括图片数据（获取方式见src/datasets/数据说明.md），及相应的图像文字标注数据（src/datasets/dataset_flickr8k.json)。



#### 数据处理

- **Train & Val & Test Split.**  utils.py文件实现了数据预处理过程，根据Flickr8k中各图像的描述将图像划分成训练、验证和测试集。

- **Word Embedding.**  使用collection库的`Counter()`计数器，统计训练集图像标注中出现的所有单词词频，对单词出现次数大于一定阈值（实现中暂定为5）的单词进行保留。为单词顺序编号，并加入四个特殊词`<unk> <start> <end> <pad>`，创建单词到编号的`wordmap`。
  - `<unk>`  未知词，用于代替所有未出现在词表中的词
  - `<start>` Caption的开始标志
  - `<end>`  Caption的结束标志
  - `<pad>`  填充，将所有句子填充成相同长度

- **Image Normalize.** 对于一张三通道的模型，我们需要通过Encoder得到一个高维的特征表达，我们使用在ImageNet上预训练模型`resnet50`作为Encoder，对于图片输入需要做一些处理：
  1. 将训练集、验证集、测试集图片resize到(3,256,256)，生成3各HDF5文件。
  2. 创建三个对应的JSON文件，对应训练集，验证集和测试集的字幕，字母顺序和HDF5文件中图片顺序一致。每张图片有5个字幕，每句字幕都被统一到52的长度。

- **Caption Encoding for Training&Val Dataset.**  对于训练集和验证集中已知的图像标注语句产生句子的Embedding。

  每个Caption的组成包括：

  	1. `<start>`
   	2. 每个单词的wordmap编号，若单词不在wordmap中，使用`<unk>`代替
   	3. `<end>`
   	4. 填充`<pad>`直至最大句子长度。

  

## Encoder

编码器的主要作用是将一张输入的3通道图片编码成固定格式，作为对原始图片的特征表述。考虑到是对图像进行特征抽取，故自然选择CNN作为编码器。目前在实现中我们选择使用torchvision中与训练的ResNet-50模型，并微调其中部分参数来提升模型性能。该模型通过逐渐的卷积和池化操作，使得图片特征越来越小，通道数越来越多，以表达语义。

CNN模型常用来进行图像识别，而此处我们仅需抽取特征，故删除最后两层（池化层和全连接层）。增加`nn.AdaptiveAvgPool2d()`函数，将特征转换到固定大小。参考对CNN的参数调整，在Encoder中加入了`freeze_params()`方法，通过该函数控制是否对Encoder中的参数进行微调，最后的特征形状为14×14×2048（2048为通道数）。

```python
class Encoder(nn.Module):
    def __init__(self, encoded_image_size=14):
        super(Encoder, self).__init__()
        self.enc_img_size = encoded_image_size
        cnn_ext = torchvision.models.resnet50(pretrained = True)  # 使用预训练的 resnet-50
        modules = list(cnn_ext.children())[:-2]  # 去掉网络中的最后两层
        self.cnn_ext = nn.Sequential(*modules)  # 定义好 encoder
        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))  # 将输出改变到指定的大小

    def forward(self, img):
        out = self.cnn_ext(img)  # [bs, 2048, 8, 8]
        out = self.adaptive_pool(out)  # [bs, 2048, enc_img_size, enc_img_size]
        out = out.permute(0, 2, 3, 1)  # [bs, enc_img_size, enc_img_size, 2048]
        return out

    def freeze_params(self, freeze):
        for p in self.cnn_ext.parameters():
            p.requires_grad = False
        for c in list(self.cnn_ext.children())[5:]:
            for p in c.parameters():
                p.requires_grad = (not freeze)
```

在test.py中，我们使用数据集中一张图片对Encoder的正确性进行了测试，正确产生了14×14×2048维度的图像特征。



## Attention

Attention机制实现相对容易，仅包含先行层和激活函数。

难点在于对模型矩阵运算维度的把握，实现过程中多次在向量维度方面出现bug。

首先通过2个线性层将Encoder得到的特征向量和Decoder中的隐藏层向量转化成相同的维度（此处实现均以batch_size为基础，批处理），在此基础上对二者相结合，经过relu函数进行激活，提供非线性。将得到的结果再次经过一个线性层和激活函数softmax，得到各batch_size大小的图片中各像素点的权重alpha，权重即可理解为Attention机制对图像各部分的关注度。最后将经过Encoder得到的图像特征与Attention机制得到的权重alpha相乘，即得到经过注意力机制筛选后的特征图像。

如图展示了Attention机制forward内容的基本实现，注释中写明了各阶段的维度变化。

```python
    def forward(self, encoder_out, decoder_hidden):
        """
        注意力机制的前向传播过程

        :param encoder_out: 提取的图片特征，大小是 (bs, num_pixels, encoder_dim)
        :param decoder_hidden: 前一步的解码输出，大小是 (bs, decoder_dim)
        :return: 注意力编码的权重矩阵
        """
        att1 = self.encoder_att(encoder_out)  # 用 self.encoder_att 作用 encoder_out, (bs, num_pixels, attention_dim)
        att2 = self.decoder_att(decoder_hidden)  # 用 self.decoder_att 作用 decoder_hidden, (bs, attention_dim)
        att2 = att2.unsqueeze(1)  # 使用 unsqueeze 将 att2 的维度从 (bs, attention_dim) -> (bs, 1, attention_dim)
        att = att1 + att2  # 将 att1 和 att2 求和，这里利用了 broadcast 的机制, (bs, num_pixels, attention_dim)
        att = self.relu(att)  # 用 relu 作用 att，提供非线性
        att = self.full_att(att)  # 用 self.full_att 作用 att，将维度映射到 1, (bs, num_pixels, 1)
        att = att.squeeze(2)  # 使用 squeeze 将 att 维度从 (bs, num_pixels, 1) -> (bs, num_pixels)
        alpha = self.softmax(att)  # 使用 self.softmax 得到每个 pixel 的权重
        # encoder_out 和注意力矩阵进行加权求和
        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(1)  # (bs, encoder_dim)
        return attention_weighted_encoding, alpha
```



## Decoder

解码器的实现代码在models.py中的DecoderWithAttention，目前我们实现了不含Attention机制的Decoder主体框架，包括定义decoder中需要的网络层，初始隐藏层的初始化，和基本的向前传播机制forward。

1 隐藏层初始化：在RNN的实现中，我们使用了LSTMCell，模型需要传入初始的hidden state和cell state，在一般的LSTM中，我们可以将其初始化为0，但现在我们有Encoder的输出，考虑利用该输出作为LSTM的hidden state 和 cell state，以提升性能。

2 词嵌入表示：根据数据预处理中得到的word_map，将每个单词用word_map中的数字代表，即整个描述语句为一个向量，其中每个数字代表一个词。以该特征向量为基础，调用nn.Embedding方法获得词嵌入表示。

3 对输入图像和词嵌入向量按照每个图像的描述语句长度进行级那个序排列，给更长的语句更高的权重，对它们处理更多次，对更短的语句更低的权重，对它们处理较少次。按时间步取小批次进行LSTM训练。



## Future

后一阶段我们将进一步完善Encoder, Attention, Decoder三大模块，尝试在小数据集上进行模型训练和预测。若时间允许，我们会进一步尝试图像物体识别，提升模型效果。





# Phase 2 - 2020/11/20

本阶段主要完成了Attention，并将之融合入Decoder部分中，在此基础上实现了Encoder、Decoder、Attention三部分的整合和模型的训练过程。



## Decoder

在上一阶段的实现中，已基本梳理出了Decoder的框架，本阶段在实现了Attention后，将Attention机制融入Decoder，完成了Decoder部分的基本功能。



![Decoder With Attention](https://github.com/Guan-JW/ai20projects/blob/master/2018202177/pics/decoderWithAttention.png?raw=true)

#### 1 定义&初始化Decoder网络层

首先定义前向传播过程中的网络层，包括词嵌入层embedding（使用torchvision中的nn.Embedding函数实现）、dropout层用于避免过拟合、LSTMCell层即Decoder的主体RNN，隐藏层、激活函数、全连接层、注意力Attention机制等。

在实现中使用了LSTMCell模型取代LSTM过程，因为LSTMCell可以按单个时间步进行处理，返回更新的隐藏层，而LSTM模型会连续迭代多个时间步，一次返回多个结果。

#### 2 forward前向传播过程

由于训练过程为批量训练，故输入参数均为batch_size为第0维度的张量。首先把输入的captions按长度降序排序，以避免对短caption尾部无用的<pads>做多余处理。

循环处理图像标注caption中的每一个词，在每个时间步使用attention机制得到对当前图像各像素点的关注权重alpha和经过alpha加权处理后的特征图像。根据论文，对隐藏层进行线性变换，并使用sigmoid函数激活，得到过滤层gate，使用gate对特征图像进一步过滤，以帮助模型更多注意到图像中的实际物体。

将过滤得到的特征图像和当前时间步中的所有词向量编码相结合，使用LSTMCell训练产生新的隐藏层，再对隐藏层附加激活函数得到预测出词汇表中各单词的概率。

核心循环过程如下：

```python
        # 在每个时间步，通过注意力矩阵和 decoder 上一步的 hidden state 来生成新的单词
        for t in range(max(decode_lens)):
            # 决定当前时间步的 batch_size，通过 [:batch_size_t] 可以得到当前需要的 tensor
            batch_size_t = sum([l > t for l in decode_lens])   # 降序处理使得循环时间缩短，短caption权重低，长caption权重高
            # 通过注意力机制得到注意力加权的 encode_out
            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],  h[:batch_size_t])
            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # 根据公式计算 soft attention 结果
            attention_weighted_encoding = gate * attention_weighted_encoding    # 过滤

            # 前向传播一个时间步，输入是 embeddings 和 attention_weighted_encoding 合并起来，可以使用 torch.cat
            # hidden state 和 cell state 也需要输入到网络中，注意使用 batch_size_t 取得当前有效的 tensor
            h, c = self.decode_step(
                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),
                (h[:batch_size_t], c[:batch_size_t]))
            preds = self.fc(self.dropout(h))  # 对 h 进行 dropout 和全连接层得到预测结果

            predictions[:batch_size_t, t, :] = preds
            alphas[:batch_size_t, t, :] = alpha
```



## Training

分批次进行训练，按batch_size读入图像和对应标注数据。

由于模型产生的是一个单词序列，使用交叉熵损失函数计算误差。

默认最大训练迭代次数epoch=120。但如若训练过程中连续8各epoch的性能没有改善，则降低学习率，继续训练；如若训练过程中连续20各epoch没有性能改善，则直接停止训练。

前向传播过程：调用Encoder得到特征图像，再经过Decoder得到预测结果。

误差计算：交叉熵损失函数计算损失。在前述Decoder过程中，每一个时间步得到的单词预测结果，为整个单词表所有单词的概率值。使用如下交叉熵函数求得所有单词预测结果的误差和。
$$
-{\sum_{i}t_ilog(y_i)+(1-t_i)log(1-y_i)}
$$
反向传播：loss.backward()更新参数。为防止梯度爆炸现象，使用nn.utils.clip_grad_value_方法进行梯度裁剪，再进一步更新模型参数。



## Validation

每训练一个epoch后，都使用当前训练得到的最好模型在验证集上进行测试，测试过程同Training过程基本一致。



## 模型训练

目前考虑到计算资源问题，仅在Flickr8k数据集上进行了模型训练。共训练了48个epochs，在validation集上的验证准确率可达到70%。

![image-20201119204230244](C:\Users\Thinkpad\AppData\Roaming\Typora\typora-user-images\image-20201119204230244.png)



## 结果测试

#### 字幕注意力热力图可视化

在Decoder的每一次迭代预测单词的过程中，均经过了Attention机制求得每个pixcel的权重alpha，保存alpha以便对每个单词的注意力进行可视化。首先将alpha与输入图像大小想对齐，使用高斯滤波对alpha的热力图效果进行平滑处理，覆盖在原始图像上。

#### 输出结果

随机输入图像测试结果，并将字幕及其注意力热力图进行可视化，得到结果如下。

![1](D:\A_Routine\AI\a-PyTorch-Project-to-Image-Caption\rst\1.png)





![2](D:\A_Routine\AI\a-PyTorch-Project-to-Image-Caption\rst\2.png)

![3](D:\A_Routine\AI\a-PyTorch-Project-to-Image-Caption\rst\3.png)

![7](D:\A_Routine\AI\a-PyTorch-Project-to-Image-Caption\rst\7.png)

![6](D:\A_Routine\AI\a-PyTorch-Project-to-Image-Caption\rst\6.png)

![5](D:\A_Routine\AI\a-PyTorch-Project-to-Image-Caption\rst\5.png)



## Staged Summary

初步的模型已初见成果，根据结果可知，对于主体明确、内容简单的图像，输出的文字标注较为准确，但对于图片中个体较多、内容相对复杂的图像，文字标注的结果不够准确，且大多数文字标注的语法相对简单。初步猜测问题来源于数据集小、训练强度不够，CNN特征提取部分不够具有针对性，Attention机制无法很好地将注意力集中在图中主要物体上。



## Future

对当前模型进行进一步调优，并尝试在更大的数据集上测试效果。

若时间允许，考虑Objective部分中提到的另外两篇论文（***Spatial and Channel-wise Attention in Convolutional Networks***和***Bottom-UP and Top-Down Attention***），对CNN特征提取部分进行改进，加入图像物体检测，使Attention更有针对性。

